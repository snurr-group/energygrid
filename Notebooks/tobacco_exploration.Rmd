---
title: "Retraining and exploration of the ToBaCCo MOFs"
author: "Ben Bucior"
date: "November 9, 2017"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(stringr)

library(purrr)
library(readxl)
library(magrittr)

library(testthat)

library(caret)
library(glmnet)

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

theme_set(theme_bw(base_size=16) + theme(aspect.ratio=1))  # Default ggplot2 params
```

## Data import and processing

```{r load.data, include=FALSE}
h2_types <- paste0("h2.", c("g.L", "mol.kg", "wtp"))
tobacco_data <- read_xlsx(
  "BigData/CrystGrowthDesign_SI.xlsx",
  sheet = "data",
  skip = 3, na = "inf",
  col_names = c(
    "MOF.ID",
    "vf", "vsa", "gsa", "pld", "lcd",
    paste0(h2_types, ".100.77"),
    paste0(h2_types, ".100.130"),
    paste0(h2_types, ".100.200"),
    paste0(h2_types, ".100.243"),
    paste0("h2.qst.6.", c(77, 130, 200, 243)),
    paste0("ch4.", rep(c("v.v.", "mg.g."), 2), c("100.298", "100.298", "65.298", "65.298")),
    "ch4.qst.6.298",
    paste0("xe.kr.1.", c("xe", "kr", "select")),
    paste0("xe.kr.5.", c("xe", "kr", "select")),
    "topology",
    paste0("n1.", c("sym", "character", "ID")),
    paste0("n2.", c("sym", "character", "ID")),
    "cbb.ID"
    )
  )
tobacco_codes <- read_table2("BigData/mofs_map.dat", col_names = c("MOF.ID", "python.id"), col_types="ic")

tobacco_data <- tobacco_data %>% left_join(tobacco_codes, by="MOF.ID")

tobacco_data <- tobacco_data %>% 
  mutate("n1.name" = str_c("sym", n1.sym, ifelse(n1.character=="organic", "on", "mc"), n1.ID, sep="_")) %>% 
  mutate("n2.name" = str_c("sym", n2.sym, ifelse(n2.character=="organic", "on", "mc"), n2.ID, sep="_"))

# Translate between Scotty's ID and the traditional ToBaCCo filename
scotty_codes <- read_tsv("BigData/tobacco-20171114/key.tsv", col_names = c("filename", "id"), col_types="cc")
scotty_codes <- scotty_codes %>% mutate(python.id = str_sub(filename, 1, -5))  # trim .cif suffix
tobacco_data <- tobacco_data %>% inner_join(scotty_codes, by="python.id")

DATA_SPLIT <- c(0.4, 0.4, 0.2)
R_GAS_KJ <- 8.314 / 1000
source("R/regression_and_features.R")  # Get regression utilities
```

```{r load_low_p}
# Also import the new low pressure data

# Update 11/22: also get 2 bar, 77 K data (incomplete GCMC)
low_p_h2_data <- read_table2(
  "BigData/Emails/tobacco-2bar-incomplete-20171121/comb-volume.txt",
  col_names = c("tob.num", "id", "h2.v.v.2.77", "h2.err.v.v.2.77"),
  col_types = "icnn"
)

low_p_h2_data <- low_p_h2_data %>% 
  mutate(`h2.g.L.2.77` = `h2.v.v.2.77` * 2.0 / 22.4) %>% 
  mutate(`h2.err.g.L.2.77` = `h2.err.v.v.2.77` * 2.0 / 22.4)
tobacco_data <- tobacco_data %>% left_join(low_p_h2_data, by="id")
# Warning: ToBaCCo data will contain rows with missing deliverable capcity.
# Training and testing code will need to handle these NAs.
tobacco_data <- tobacco_data %>% mutate(h2.deliv.77 = h2.g.L.100.77 - h2.g.L.2.77)


# 77 K / 6 bar from EES paper, Yamil, 2017-11-15
# Similar format to the CrystGrowthDesign spreadsheet above, but with fewer columns to import
low_p_h2_data_raw <- read_xlsx(
  "BigData/Emails/tobacco-yamil-20171115/EES-SI-07-18-2016.xlsx",
  sheet = "data",
  skip = 3, na = "inf",
  col_names = c(
    "MOF.ID",
    "vf", "vsa", "gsa", "pld", "lcd",
    paste0(h2_types, ".100.77"),
    paste0(h2_types, ".5.160"),
    paste0(h2_types, ".6.77"),
    paste0("h2.qst.", c("6.77", "5.160")),
    "topology",
    paste0("n1.", c("sym", "character", "ID")),
    paste0("n2.", c("sym", "character", "ID")),
    "cbb.ID"
    )
  )
# tobacco_data[1,1] = 234052409235  # Tested the next piece of test code by messing up one of the ID's
# Ensure data consistency: that compositions for the first MOF.ID column are the same
# anti_join will look for rows that mismatch within these columns
temp_mof_id_mismatches <- low_p_h2_data_raw %>%
  anti_join(tobacco_data, by=c(
    "MOF.ID", "topology",
    paste0("n1.", c("sym", "character", "ID")),
    paste0("n2.", c("sym", "character", "ID"))
    )) %>% 
  nrow
expect_equal(temp_mof_id_mismatches, 0)  # Now we can safely use MOF.ID to join
# Alternatively, we could have just performed an inner join on all of these columns
low_p_h2_data <- low_p_h2_data_raw %>% 
  select(c(1, 10:15, 17))  # MOF.ID and the new H2 data (not 100 bar cryo)
tobacco_data <- tobacco_data %>% inner_join(low_p_h2_data, by="MOF.ID")
#tobacco_data <- tobacco_data %>% mutate(h2.deliv.77 = h2.g.L.100.77 - h2.g.L.6.77)

# Let's also grab CH4 data at 6 bar (noting that this file also provides H2 at 6 bar, 243 K)
raw_ch4_6_bar <- read_xlsx(
    "BigData/Emails/tobacco-yamil-20171115/6bar.xlsx",
    sheet = "CH4",
    skip = 1, na = "inf",
    col_names = c("MOF.ID", paste0("ch4.", c("v.v", "mg.g"), ".6.298"))
    )
tobacco_data <- tobacco_data %>% inner_join(raw_ch4_6_bar, by="MOF.ID")
```

```{r load_grids}
if (!exists("raw_grids_h2")) {
  raw_grids_h2 <- read_rds("BigData/Robj/tobacco_h2.Rds")
  # TODO: we should calculate the base histogram with less granularity (0.05 kJ/mol instead of 0.01) to considerably reduce file and processing size
}
```

```{r partition_data}
tob_y_to_join <- tobacco_data %>% 
  rename(g.L = h2.deliv.77) %>% 
  select(id, g.L) %>% 
  filter(!(is.na(g.L) | g.L < 0))  # Clean up unphysical MOFs
complete_ids <- raw_grids_h2 %>% 
  select(id) %>% 
  unique() %>% 
  inner_join(tob_y_to_join, by="id") %>% 
  select(id) %>% 
  unlist

tob_y_to_join <- tob_y_to_join %>% filter(id %in% complete_ids)
grids_h2 <- raw_grids_h2 %>% filter(id %in% complete_ids)

tob_hist_sets <- partition_data_subsets(grids_h2, tob_y_to_join, DATA_SPLIT)
```

Long-term, we also may consider moving this to a modular, independent R script to generate the figures all separately.  (Save models as R objects and import them individually as necessary, then ggsave the plots as their publication-ready formats)

## Proof-of-concept modeling
Let's run a quick ridge regression model on the ToBaCCo MOF set to see if the performance is similar to the hMOF results presented at AIChE.

Note: had to temporarily test with the hMOFs instead of ToBaCCo: the ToBaCCo MOFs lack sufficient diversity in volumetric uptake: they're considerably higher than the hMOFs, but also more tightly clustered which prevents the model from sufficiently generalizing.  Training on hMOFs and testing on ToBaCCo MOFs works great, unless we use some advanced sampling techniques on the ToBaCCo MOFs.

```{r model_proof_of_concept}
source("R/plot_hists.R")
source("R/regression_and_features.R")
source("R/get_energy_stats.R")
source("R/plot_diagnostics.R")

source("R/load_data.R")
# Also load CH4 data for the hMOFs from Scotty.  We don't have this data from Wilmer's work, since they were only concerned with absolute uptake at that point.
raw_2500_cols <- c("kJ.mol", "cm3.cm3", "g.kg")
raw_ch4_hmof <- read_tsv(
  "BigData/2500hmof-data/2500hmofdata.tsv",
  skip = 2,
  na = c("", "#NAME?", "-"),
  col_names = c(
    "id",
    paste("h2", raw_2500_cols, "2.160", sep="."),
    paste("ch4", raw_2500_cols, "5_8.298", sep="."),
    paste("ch4", raw_2500_cols, "65.298", sep=".")
    ),
  col_types = "innnnnnnnn" # TODO FINISH
  )
# Note: some columns only have a "#NAME?" flag in the kJ/mol column, but zero in the other two, so clean them up here:
# deletes rows without any GCMC and erroneous zero entries associated with a corresponding NA
raw_ch4_hmof <- raw_ch4_hmof %>% 
  select(-starts_with("h2")) %>% 
  na.omit() %>% 
  full_join(na.omit(select(raw_ch4_hmof, id, starts_with("h2"))), by="id")
raw_ch4_hmof <- raw_ch4_hmof %>% 
  mutate(h2.g.L.2.160 = h2.cm3.cm3.2.160 * 2.0 / 22.4)
gcmc_data <- gcmc_data %>% 
  left_join(raw_ch4_hmof, by="id")

hmof_h2_grid <- read_rds("BigData/Robj/hmof_h2.Rds")
hmof_h2_grid <- hmof_h2_grid %>%
  mutate(str_id = str_sub(id, 6, -1)) %>% # strip the hMOF- prefix
  mutate(id = as.integer(str_id)) %>% 
  select(-str_id)
hmof_y_to_join <- gcmc_data %>% 
  select(id, g.L) %>% 
  filter(!(is.na(g.L) | g.L < 0))
hmof_hist_sets <- partition_data_subsets(hmof_h2_grid, hmof_y_to_join, DATA_SPLIT)
# Note: since we're starting with 2500 hMOFs, 40% for training is still 1000, so that works out great.

# First, let's plot a ridge regression model against ToBaCCo
temp_tob_part_mod <- run_model_on_partitions(tob_hist_sets, tob_y_to_join, default_binspec, 0)
print(temp_tob_part_mod)
parity_plot(temp_tob_part_mod$trained_mod$y, pred_glmnet(temp_tob_part_mod$trained_mod, temp_tob_part_mod$trained_mod$orig_x), "#CA7C1B", alpha=0.5)

# Final saved results are ridge (alpha = 0)
for (perf_alpha in c(1, 0)) {
  hmof_partitioned_mod <- run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, default_binspec, perf_alpha)
  print(hmof_partitioned_mod)
  trained_model <- hmof_partitioned_mod$trained_model
  trained_mod <- hmof_partitioned_mod$trained_mod
}
plot(hmof_partitioned_mod$plots$parity_bw)
```

As a diagnostic, I tried training the ToBaCCo data on 6 bar, 77 K data that may have more accurate GCMC.  The result is a different level of fit than previously.  Q2 has shot up from 0.04 to 0.42, but RMSE increased from 4.7 to 6.0, and MAE from 3.1 to 3.5.  There's also quite a few MOFs now on the peripheries of the model, either in the upper 30's g/L predicted or GCMC lines.


```{r tobacco_diagnostic}
# Try the hMOF model on the ToBaCCo data for sampling
# TODO: we could likely rewrite this in terms of pred_grid and eval_test_grid, but it's a one-off type analysis

# First, how does its distribution of y values vary?  I think the model above is overfitting, given its mean capacity of 38 g/L
hist(tob_y_to_join$g.L)

# COPIED FROM ABOVE: MAKE THIS A SUBROUTINE.
# MAYBE WE NEED A MODEL SETUP OBJECT, INCLUDING BINS AND OTHER PARAMS?
# Performance on the test data, which hasn't yet been used for anything
testing_desc <- tob_hist_sets$testing %>% 
  stepped_hist(1.0, 1.0, -20, 1.0) %>% 
  spread(key=bin, value=metric)
y_act <- testing_desc %>%
  left_join(tob_y_to_join, by="id") %>% 
  rename(y = g.L) %>% 
  .$y
testing_ids <- testing_desc %>% select(id)
testing_desc <- testing_desc %>% select(-id)

y_pred <- pred_glmnet(trained_mod, testing_desc)
postResample(pred=y_pred, obs=y_act) %>% print

# NEW FOR COLORING
has_hmof_topology <- testing_ids %>%
  select(id) %>% 
  left_join(tobacco_data, by="id")
has_hmof_topology <- (has_hmof_topology$topology %in% c("pcu", "sra", "diab", "tbo", "nbo", "fcu"))

p <- parity_plot(y_act, y_pred, ifelse(has_hmof_topology, "red", "gray"), 0.25)
p <- p + ylab(paste0("Predicted uptake (ridge regression)"))
print(p)

hist(y_act - y_pred)

  
  
# For diagnosis, we might consider plotting the various distributions to see how similar they are between the hMOFs and ToBaCCo MOFs
standardize(trained_mod, testing_desc) %>% .$`Inf` %>% hist

# Compare z/y/beta plots for the training hMOFs and test ToBaCCo MOFs
plot_bin_z_vs_y(trained_mod$x, trained_mod$y, coef_tbl(trained_mod$mod))
plot_bin_z_vs_y(standardize(trained_mod, testing_desc), y_act, coef_tbl(trained_mod$mod))
# What happens if we compare against the model instead of GCMC?  What does the "Inf" bin look like?
plot_bin_z_vs_y(standardize(trained_mod, testing_desc), y_pred, coef_tbl(trained_mod$mod))

# Finally, what are the distributions of z-scores (and actual values) between ToBaCCo and hMOFs?
x_distr <- function(x) {
  x %>% 
    gather(key="bin", value="value") %>% 
    ggplot(aes(value)) +
    geom_histogram() +
    facet_wrap(~bin, scales="free") +
    theme(text = element_text(size = 8))
}

x_distr(trained_mod$x)
x_distr(standardize(trained_mod, testing_desc))
# And the non-trainsformed versions
x_distr(select(trained_mod$orig_x, c("17", "18", "19", "20", "21", "Inf")))
x_distr(select(testing_desc, c("17", "18", "19", "20", "21", "Inf")))

#How do the distributions of the top-performing MOFs compare?
testing_desc %>%
  bind_cols(id = testing_ids$id) %>% 
  left_join(tob_y_to_join, by="id") %>% 
  rename(y_act = g.L) %>% 
  filter(y_act > 45) %>% 
  select(c("17", "18", "19", "20", "21", "Inf")) %>% 
  x_distr()
```

What ToBaCCo MOFs have the most egregious error?

```{r top_tobacco_err}
which(abs(y_pred - y_act) > 10) %>% length
length(y_pred)  # okay, so it's a pretty big number of them
testing_ids %>% 
  bind_cols(y_pred = as.numeric(y_pred), y_act = y_act) %>% 
  mutate(abs_err = abs(y_pred - y_act)) %>% 
  arrange(desc(abs_err)) %>% 
  inner_join(tobacco_data, by="id") %>% 
  select(id, abs_err, y_pred, y_act, filename) %>% 
  View
  # filter(abs_err > 10) %>% write_tsv("tob_err_gt_10.tsv")
  # Can export these results then copy the relevant energy files from turquoise at finer grid spacing
  # mkdir ../fine; for i in $(tail -n +2 ~/tob_err_gt_10_20171122.tsv | cut -f1); do cp -r $i ../fine; done; rm ../fine/*/{atomcoords.txt,Energy_Values.txt,*.cif}

# Now let's take another look at the worst errors with a finer grid of 0.5 Angstrom.  Is it a grid spacing convergence issue or something potentially more fundamental?
test_tob_conv_grids <- read_rds("BigData/Convergence/top_20171122.Rds")
print("Original 1.0 Angstrom grid")
test_conv_100 <- tob_hist_sets$testing %>% 
  filter(id %in% unique(test_tob_conv_grids$id)) %>% 
  eval_test_grid(trained_mod, ., default_binspec, rename(tob_y_to_join, y_act=g.L))
print(test_conv_100)
print("Finer 0.5 Angstrom grid")
test_conv_50 <- test_tob_conv_grids %>% 
  eval_test_grid(trained_mod, ., default_binspec, rename(tob_y_to_join, y_act=g.L))
print(test_conv_50)
print("Comparison")
test_conv_diff <- test_conv_100$pred_df %>% 
  select(id, y_pred) %>% 
  rename(y_100 = y_pred) %>% 
  full_join(test_conv_50$pred_df, by="id") %>% 
  rename(y_50 = y_pred) %>% 
  select(-y_err)
test_conv_diff %>% 
  ggplot(aes(y_50, y_100)) +
  geom_point()
test_conv_diff %$% hist(y_100 - y_50)
```


## Revisiting AIChE figures
At any rate, we should see what the hMOF model looks like for betas, multiple pressures, different gases, etc.

```{r hmof_betas}
# Deliverable capacity
source("R/color.R")
base_hist <- hmof_h2_grid %>% 
  filter(id == 71) %>% 
  plot_hist_bins(default_binspec)

base_hist %>% overlay_cat_betas(coef_tbl(trained_mod$mod), default_binspec)

# New figure 11/17 to look at effect of PLD, simplified on 11/20
gcmc_data %>% 
  mutate(y_act = g.L) %>% 
  run_model_on_partitions(hmof_hist_sets, ., default_binspec, 0) %>% 
  .$pred_df %>% 
  ggplot(aes(y_err, dpd)) + geom_point()

# Individual components on hydrogen (repeat the full model analysis workflow)
p_hi_model <- hmof_y_to_join %>% 
  select(id) %>% 
  left_join(gcmc_data, by="id") %>% 
  select(id, va100bar) %>% 
  rename(g.L = va100bar) %>% 
  run_bin_model(hmof_hist_sets$training, ., default_binspec["step"], default_binspec["width"], default_binspec[c("from", "to")])

p_lo_model <- hmof_y_to_join %>% 
  select(id) %>% 
  left_join(gcmc_data, by="id") %>% 
  select(id, va2bar) %>% 
  rename(g.L = va2bar) %>% 
  run_bin_model(hmof_hist_sets$training, ., default_binspec["step"], default_binspec["width"], default_binspec[c("from", "to")])

list(`100 bar` = p_hi_model$fitted_model[[1]]$mod, `2 bar` = p_lo_model$fitted_model[[1]]$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec)

```


## New analysis
### Testing models on high pressure data only
Perhaps ToBaCCo will work better only on one pressure, since we already have data available for 100 bar for both hMOFs and ToBaCCo.  Let's see how the model looks while waiting to fill in the low pressure 2 bar data (since we only have 6 bar available).

```{r transfer_high_p}
library(magrittr)
p_hi_mod <- p_hi_model$fitted_model[[1]]

hi_testing_tob <- tobacco_data %>% 
  mutate(y_act = h2.g.L.100.77) %>% 
  filter(!(is.na(y_act) | y_act <0)) %>%  # Clean up unphysical MOFs
  eval_test_grid(p_hi_mod, tob_hist_sets$testing, default_binspec, .)
print(hi_testing_tob)
#hi_y_pred <- pred_glmnet(p_hi_mod, hi_testing_desc)

hi_data <- hi_testing_tob$pred_df %>% 
  mutate(has_hmof_topology = (topology %in% c("pcu", "sra", "diab", "tbo", "nbo", "fcu")))
hi_data %$%  # pipe the names from this tibble
  parity_plot(y_act, y_pred, color=ifelse(has_hmof_topology, "red", "gray"), alpha=0.3)
hi_data %$%
  hist(y_act - y_pred)

hi_data %>% nrow
hi_data %>% 
  filter(y_act > 45 & y_pred < 40) %>% 
  select(id, topology, filename)
```

### Consistency between overlapping structures (MOF-5)
One of the best diagnostics will be inspecting MOFs that we expect to be in common between the two databases (e.g. hMOF-0 and tobmof6018).  Later, we could consider expanding this to a whole set of MOFs. in common, if of interest.

```{r db_consistency}
ad_hoc_h2_grid <- read_rds("BigData/Robj/ad_hoc.Rds")
# id "Bad" refers to a corrupted CIF for a completely different MOF.
plot_mof_minimal(ad_hoc_h2_grid, "hypotheticalMOF0", default_binspec)
plot_mof_minimal(ad_hoc_h2_grid, "pcu_sym_6_mc_3_L_12", default_binspec)

# First try a density plot.  The stats are acting strange, so let's generate it manually
#ad_hoc_h2_grid %>%
#  group_by(id) %>% mutate(dens = counts / sum(counts)) %>% ungroup() %>%
#  ggplot(aes(lower, weight=dens, col=id)) +
#  stat_density(adjust = 1/5, size = 2, geom = "line")

# Next let's look at the hist bins directly, so we can also see what's going on with "Inf"
ad_hoc_h2_grid %>% 
  stepped_hist_spec(default_binspec) %>% 
  mutate(height = metric) %>% 
  inner_join(color_from_binloc(bin_loc_from_spec(default_binspec)), by="bin") %>% 
  ggplot(aes(loc, height, col=id)) +
  #geom_line(size = 1.5, alpha = 0.7) +
  geom_point(alpha = 0.7) + geom_line(size=0.5) +
  labs(x="Energy (kJ/mol)")
# What are the 100 bar uptakes for these MOFs?
gcmc_data %>% filter(id == 0) %>% select(va100bar)
tobacco_data %>% filter(filename=="pcu_sym_6_mc_3__L_12.cif") %>% select(h2.g.L.100.77)

# What about the predictions on these MOFs?
ad_hoc_raw_bins <- ad_hoc_h2_grid %>% 
  filter(id != "bad") %>% 
  stepped_hist_spec(default_binspec) %>% 
  spread(key=bin, value=metric)
ad_hoc_raw_bins
ad_hoc_raw_bins %>% 
  select(-id) %>% 
  pred_glmnet(p_hi_mod, .)

# How consistent is the number of grid points/samples on these two materials?
ad_hoc_h2_grid %>% 
  group_by(id) %>% 
  summarize(sum(counts))
```

### Plotting by color
Next, let's see if we can figure out the discrepancy in high pressure loading from a graphical argument.  We have several MOF textural properties in the tobacco_data spreadsheet.  Based on the previous block, it also might be worthwhile double checking the level of sampling for each MOF.  This block will use the model and data mentioned in the block `transfer_high_p` above.


```{r plot_err_trends}
hi_data_w_sampling <- tob_hist_sets$testing %>% 
  group_by(id) %>% 
  summarize(sampling = sum(counts)) %>% 
  ungroup %>% 
  select(id, sampling) %>% 
  inner_join(hi_data, by="id")

summary(hi_data$sampling)
# Manually use parity plot so we have more fine grained control

hi_data_w_sampling %$%
  qplot(y_act, y_pred, col=ifelse(sampling > 30000, 30000, sampling), alpha=I(0.4)) +
  xlab("'Actual' uptake (GCMC simulations)") +
  ylab("Predicted uptake (ridge regression)") +
  expand_limits(x = 0, y = 0) +
  scale_x_continuous(limits = c(0,70)) +
  scale_y_continuous(limits = c(0,70)) +
  scale_color_gradientn(colors=c("red", "black", "blue"), name=NULL) +
  parity_line
# Based on this analysis, let's revisit the apparently worst points
hi_data_w_sampling %>% 
  arrange(desc(y_err)) %>% 
  select(id, y_err, y_pred, y_act, filename, sampling) %>% 
  View
# Alternatively,
hi_data_w_sampling %>% 
  mutate(sampling = ifelse(sampling > 30000, 30000, sampling)) %>% 
  ggplot(aes(y_err, sampling, col=vf)) +
  geom_point()
# Or this way, with two plots
hi_data_w_sampling %>% 
  mutate(num_pts = if_else(sampling < 15000, "Low number of points", "At least 25^3")) %>% 
  ggplot(aes(y_act, y_pred, col=y_err)) +
  geom_point() + 
  facet_wrap(~num_pts) +
  parity_line
```

This plot reminds me that there's a simple way to increase the number of variables I can view simultaneously, especially when factors are involved: summary!  However, it's hard to compare proportions to look for differences in representativenesss, other than noticing things like the null linker being a problem.

```{r summary_err_hi}
hi_err_for_summary <- hi_data %>% 
  select(y_err, vf, n1.name, n2.name, cbb.ID, pld, vsa) %>% 
  mutate_at(3:5, factor)
cat("Full set of tested ToBaCCo MOFs:\n")
summary(hi_err_for_summary)
cat("ToBaCCo MOFs with high error (> 10g/L):\n")
summary(filter(hi_err_for_summary, y_err > 10))
```

Now let's look at a few other plots I was intending to view.
```{r misc_plots}
ggplot(hi_data, aes(y_err, vf)) + geom_point()
ggplot(hi_data, aes(y_err, vsa)) + geom_point()
ggplot(hi_data, aes(y_err, pld)) + geom_point()

# But if it's a convergence issue, why don't we see it as bad in the hMOFs?
# Let's take a look at the PLD plot
# (see above.  We had to use DPD since that's all we have in the spreadsheet)
```

### Retry a single point
See revised `ad_hoc_raw_bins` definition and data.frame above. It's getting a prediction of 51.91267 vs. an actual value of 54.553571 for `ith_sym_4_on_7_sym_13_mc_12_L_40.cif`, so that's all the problem was.  For some reason, the file on the testing folder has 2197 lines versus 21952 in my version.


### Figure showing bin robustness
Eventually, we could move this to a separate function in plot_hists.R, but it doesn't seem like there's much point in doing that since we only need a single figure.

```{r bin_robusness}
if (!exists("hyper_tuned_hist")) {
  hyper_tuned_hist <-
    seq(0.25, 2.5, 0.25) %>% # eventually consider 0.25-2.5 in quarter increments
    map_dfr(
      function(x) run_bin_model(
        e_data = hmof_hist_sets$hyperparam,
        y_with_id = hmof_y_to_join,
        step = x, width = x,
        bin_lims = c(default_binspec["from"], default_binspec["to"]),
        lambda = NULL, alpha = 0
        )
      ) %>% 
    # Warning: the above output cannot be viewed without deleting the fitted_model and id_list list columns
    (function(x) {
      y <- tibble()
      for (rownum in 1:nrow(x)) {
        curr_row <- x[rownum,]
        y <- coef_tbl(curr_row[[1,"fitted_model"]]$mod) %>% 
          mutate(q2 = curr_row$q2, binwidth = curr_row$width) %>% 
          inner_join(
            bin_loc_from_spec(c(
              from=curr_row$bin_lo,
              to=curr_row$bin_hi,
              step=curr_row$step,
              width=curr_row$width
              )),
            by="bin"
            ) %>% 
          bind_rows(y, .)
      }
      y
    })(.)  # run the anonymous function
}

plot_robustness_bin_gap <- 0.05
plot_robustness_beta_max <- 5
hyper_tuned_hist %>% 
  mutate(beta = ifelse(beta > plot_robustness_beta_max, plot_robustness_beta_max, beta)) %>% 
  mutate(beta = ifelse(beta < -plot_robustness_beta_max, -plot_robustness_beta_max, beta)) %>% 
  mutate(lower = lower + plot_robustness_bin_gap, upper = upper - plot_robustness_bin_gap) %>% 
  ggplot(aes(
    x = binwidth,
    ymin = lower,
    ymax = upper,
    color = beta
  )) +
  geom_linerange(size=2) +
  coord_flip() +
  scale_color_gradientn(
    colors = c("red", "darkgray", "blue"),
    limits = c(-plot_robustness_beta_max, plot_robustness_beta_max),
    breaks = c(-plot_robustness_beta_max, 0, plot_robustness_beta_max),
    labels = c(paste0(-plot_robustness_beta_max,"-"), "0", paste0(plot_robustness_beta_max,"+")),
    name=expression(beta[ridge])
    ) +
  labs(
    y = "Energy (kJ/mol)",
    x = "Width of histogram bins (kJ/mol)"
  )
# See relevant ggplot documentation at http://ggplot2.tidyverse.org/reference/geom_linerange.html

# This figure highlights a suspicion I had, that upper bound is based on where there aren't any bins
# remaining in the binspec.  An alternate approach would be centering this to zero instead of the lower
# bound, then only including one bin (or up to bin max??) before cutting it off.

# Now ask, how consistent is the performance among the different models?
hyper_tuned_hist %>% 
  select(binwidth, q2) %>% 
  unique
# TODO: consider reporting RMSE and/or MAE, possibly from the training data?

```

Three things stand out to me as particularly noteworthy from this analysis.

1. As hypothesized/hoped, the model is quite robust to the histogram parameter selection.  As bin size decreases, the magnitude of beta decreases since the effect is spread out over multiple adjacent bins.
2. The left hand side cuts out at approximately the same place, because that's where the variation in the hMOF database goes to zero.
3. Performance, at least as characterized by Q2, slightly decreases with coarser bins, but remains quite good (0.95+).


## Model generality

### Different temperatures

Conclusion?  Limited data currently to perform this analysis (only H2 at two temperatures, 2 bar)

Let's build models for higher temperatures of hydrogen.  Are the models still predictive?  Let's check the Q2 and parity plots graphically in this document, then likely assemble a beta plot for publication.  The plot might actually work as a subfigure on the pressure breakdown for interpretability.  (first introduce the concept of beta, then deliverable as two pressures, then for several T/P combinations to look at overall trends).  This will be easier to implement than methane because the grids and histograms are exactly identical: only the y axis (g/L) changes.

This plan sounds great in theory, but we don't actually have the proper training data available for the hMOFs.  However, my suspicion on the $y$ distribution causing the training problems for ToBaCCo is actually testable: let's see how well ToBaCCo trains in a lower uptake scenario (e.g. 160 K).

Then, we can insert a block looking at the betas from different temperatures.

```{r tobacco_training_from_weak}
# First identify what distributions of absolute uptake look promising
tobacco_data %>% 
  mutate(g.L = h2.g.L.2.77) %>% 
  run_model_on_partitions(tob_hist_sets, ., default_binspec, 0)
hist(tobacco_data$h2.g.L.2.77)
# This section hints that the model is having serious trouble learning the low pressure data.  Is that also true of the hMOFs?
# Update 11/23: that's only true for the 6 bar data, not 2 bar
weak_hmof <- gcmc_data %>% 
  mutate(g.L = va2bar) %>% 
  run_model_on_partitions(hmof_hist_sets, ., default_binspec, 0)
print(weak_hmof)
hist(gcmc_data$va2bar)
# No, the hMOFs are fine, and the beta coefficients are more intuitive (negative effect of the Inf bin, etc.)

# How well does the low P hMOF model work on the ToBaCCo MOFs?
tobacco_data %>% 
  mutate(y_act = h2.g.L.2.77) %>% 
  eval_test_grid(weak_hmof$trained_mod, tob_hist_sets$testing, default_binspec, .)
```

But first, can we improve ToBaCCo predictivity by resampling before training?  Imbalanced data can potentially be a problem even for regression ([ref `krawczyk_learning_2016`](https://link.springer.com/article/10.1007/s13748-016-0094-0)), so we could consider downsampling the ToBaCCo MOFs.  But as mentioned on [stack exchange](https://stats.stackexchange.com/questions/30162/sampling-for-imbalanced-data-in-regression), there's problems with just sampling against y.

But in the meantime, there's at least some limited data available for the hMOFs.

```{r h2_hmof_2bar}
# First, we need to set up a new set of training and test data, because the 160K data are incomplete,
# and run_bin_model has not been updated to automatically handle NA cases
p_2bar_data <- na.omit(gcmc_data)  # probably overzealous, but it'll remove all of the junk
p_2bar_grids <- hmof_h2_grid %>% filter(id %in% p_2bar_data$id)
p_2bar_sets <- partition_data_subsets(p_2bar_grids, p_2bar_data, DATA_SPLIT)

print("Model for 2 bar, 77 K")
p_2bar_77k <- p_2bar_data %>% 
  mutate(g.L = va2bar) %>% 
  run_model_on_partitions(p_2bar_sets, ., default_binspec, 0)
print(p_2bar_77k)

print("Model for 2 bar, 160 K")
p_2bar_160k <- p_2bar_data %>% 
  mutate(g.L = h2.g.L.2.160) %>% 
  run_model_on_partitions(p_2bar_sets, ., default_binspec, 0)
print(p_2bar_160k)

# Now let's generate the beta plot.
# Consider adding this to the 100 bar, 77 K results?
list(`77 K` = p_2bar_77k$trained_mod$mod, `160 K` = p_2bar_160k$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec)

```

### Methane
Different grid files, but otherwise a similar analysis process.  Though strength of binding might lead to some interesting min/max analysis.

```{r hmof_methane}
# Let's use the data sets from the previous block.  There are some cases where we have methane data but not H2, and vice versa, but let's go with the MOFs in common for simplicitly.
raw_hmof_grids_ch4 <- read_rds("BigData/Robj/hmof_ch4.Rds") %>% rename(dirname = id)
p_ch4_grids <- raw_hmof_grids_ch4 %>% 
  mutate(id = as.integer(str_sub(dirname, 6))) %>% 
  filter(id %in% p_2bar_data$id)

# Expand lower limits for CH4 until the bottom bins are completely unpopulated
ch4_binspec <- c(from=-40, to=2.0, step=2.0, width=2.0)

p_ch4_sets <- partition_data_subsets(p_ch4_grids, p_2bar_data, DATA_SPLIT)
p_ch4_vol <- p_2bar_data %>% 
  mutate(g.L = ch4.cm3.cm3.65.298 - ch4.cm3.cm3.5_8.298) %>% 
  run_model_on_partitions(p_ch4_sets, ., ch4_binspec, 0)
print(p_ch4_vol)

p_ch4_vol$plots$parity_full +
  scale_x_continuous(limits = c(0,250)) +
  scale_y_continuous(limits = c(0,250))

# Now, what do the coefficients look like?
# Consider using a different colormap for the different gas?  Or at least label the figure very clearly
ch4_base_hist <- p_ch4_grids %>% 
  filter(dirname == "hMOF-71") %>%  # Using the same MOF as the sample H2 histogram
  plot_hist_bins(ch4_binspec)
ch4_base_hist %>% overlay_cat_betas(coef_tbl(p_ch4_vol$trained_mod$mod), ch4_binspec, scaling = 20.0)
# Based on this plot, we see that we need (a) separate high/low P plots for additional interpretation, and (b) should at least consider extending the range of minimum adsorption, which there's a chance may explain the poor model performance at low loading.
# Based on changes to binspec, we should consider how to incorporate that information in the model definition file.  Also what format to use for that data?  JSON for loading/interpretability?
# For help on NSE in dplyr, see vignette("programming")
library(rlang)  # see also https://github.com/tidyverse/rlang/issues/116
mutate_col_to_gL <- function(newcol) {
  mutate(p_2bar_data, g.L = !!sym(newcol))
}
#mutate_col_to_gL("ch4.cm3.cm3.65.298") %>% View
list(`5.8 bar` = "ch4.cm3.cm3.5_8.298", `65 bar` = "ch4.cm3.cm3.65.298") %>% 
  map(mutate_col_to_gL) %>%
  map(function(x) run_model_on_partitions(p_ch4_sets, x, ch4_binspec, 0)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(ch4_base_hist, ., ch4_binspec, scaling = 20.0)


# We should also calculate this on a gravimetric basis for direct comparison against Pardakhti et al., 2017
# But that will require absolute uptake data at 35 bar and 298 K, not this new deliverable data.
# TODO: Randy provided that database for UConn.  Can I also get it as a baseline for this study?
# TODO: if so, I should also calculate MAPE.  Shouldn't be difficult: it's just y_err / y_act * 100
p_2bar_data %>% 
  mutate(g.L = ch4.g.kg.65.298 - ch4.g.kg.5_8.298) %>% 
  run_model_on_partitions(p_ch4_sets, ., ch4_binspec, 0)

```

```{r ch4_tobacco}
# Repeat the CH4 hMOF analysis for ToBaCCo.  Does it behave like H2 deliverable capacity?
# First, attempt training/testing only on ToBaCCo data
grids_ch4_tobacco <- read_rds("BigData/Robj/tobacco_ch4.Rds")
tob_ch4_sets <- partition_data_subsets(grids_ch4_tobacco, tobacco_data, DATA_SPLIT)
mod_ch4_tobacco <- tobacco_data %>% 
  mutate(g.L = ch4.v.v.65.298 - ch4.v.v.6.298) %>% 
  run_model_on_partitions(tob_ch4_sets, ., ch4_binspec, 0)
print(mod_ch4_tobacco)

mod_ch4_tobacco$plots$parity_full +
  scale_x_continuous(limits = c(0,250)) +
  scale_y_continuous(limits = c(0,250))

# Then, like we had to do for the hMOFs, use the trained hMOF model on ToBaCCo.

mod_hmof_ch4_on_tob <- tobacco_data %>% 
  mutate(y_act = ch4.v.v.65.298 - ch4.v.v.6.298) %>% 
  filter(!(is.na(y_act) | y_act < 0)) %>%  # Clean up unphysical MOFs
  eval_test_grid(p_ch4_vol$trained_mod, tob_ch4_sets$testing, ch4_binspec, .)
print(mod_hmof_ch4_on_tob)
mod_hmof_ch4_on_tob$plots$parity_full +
  scale_x_continuous(limits = c(0,250)) +
  scale_y_continuous(limits = c(0,250))
```

One more analysis, based on some hypotheses that Randy semi-frequently mentions about methane adsorption.

```{r h2_vs_ch4_ads}
tobacco_data %>% 
  gather(pressure, uptake, starts_with("h2.g.L.100.")) %>% 
  ggplot(aes(ch4.v.v.100.298, uptake, color=vf)) +
  facet_wrap(~pressure, scales="free_y") +
  geom_point(alpha=0.1) +
  xlab("Methane uptake (v/v, 100 bar, 298 K)") +
  ylab("Hydrogen uptake (g/L)")

```

### More break down of ToBaCCo errors
```{r tobbaco_by_composition}
tob_deliv_from_hmof <- tobacco_data %>% 
  mutate(y_act = h2.deliv.77) %>% 
  eval_test_grid(trained_mod, tob_hist_sets$testing, default_binspec, .)

print(tob_deliv_from_hmof)

# Look at dependence on composition, topology, etc.

base_tob_deliv_p <- 
  tob_deliv_from_hmof$pred_df %>% 
  ggplot(aes(y_act, y_pred)) +
  geom_point() + 
  parity_line

base_tob_deliv_p +
  facet_wrap(~ cbb.ID) +
  theme(text = element_text(size = 4))

#tob_deliv_from_hmof$pred_df %>% 
#  group_by(topology) %>% summarize(freq = n()) %>% ungroup %>% 
#  right_join(tob_deliv_from_hmof$pred_df, by="topology") %>% 
base_tob_deliv_p +
  facet_wrap(~ topology) +
  theme(text = element_text(size = 10)) +
  theme(axis.text = element_blank(), axis.ticks = element_blank()) #+
#  geom_label(aes(x=50, y=0, label=freq), size = 4)

# Also consider training hMOFs/predictions by topology, similar linkers, etc.?
tob_deliv_from_hmof$pred_df %>% 
  group_by(topology) %>% 
  summarize(n())

base_tob_deliv_p +
  facet_grid(n1.character ~ n2.character, margins = TRUE) +
  coord_cartesian(xlim = c(0,60), ylim = c(0,60))
base_tob_deliv_p +
  facet_grid(n1.sym ~ n2.sym)

# Is performance dependent on linker/node similarity to the hMOFs?
# Hard to quantify due to differences in how the nodes and linkers are represented.
# Stats on overlap/polymorphism could be interesting for other work, though.
# tob_deliv_from_hmof$pred_df %>% 
#   mutate(
#     hmof_linker = (CBB.ID %in% c(12, 8, 10, 3, 34, 41, 40, 2, 14, 15, 17, 8, 31, 22, 16, 27, 11, 33, 7, 32, 39, 44))
#     ) %>% 
#   ggplot(aes(y_act, y_pred)) +
#   geom_point() + 
#   parity_line +
#   facet_grid(NODE_AVAIL ~ LINKER)
```

## Training hMOFs vs. ToBaCCo
Some (hopefully) final analysis behind why the ToBaCCo MOFs don't train as well.  The cause is likely rooted in the differences in x- and y- distributions, but let's diagnose a few more questions, first.

### Use ToBaCCo model on hMOFs
So far, we've been focusing on using the hMOF model to predict ToBaCCo MOFs.  How well does it work in reverse?  Meaning, is the problem with training on the diverse ToBaCCo set, too much density at high uptake, self-similarity in hMOFs, or something else?

```{r tobacco_to_hmof}
gcmc_data %>% 
  mutate(y_act = g.L) %>% 
  eval_test_grid(temp_tob_part_mod$trained_mod, hmof_hist_sets$testing, default_binspec, .)
```

These results are interesting: despite the challenges of ToBaCCo at high loading, overall the model captures the overall trend from y = 10 to max g/L (albeit underpredicting because the ToBaCCo model is trying to compensate for the overprediction on most points).  Below that, far away from the ToBaCCo density, the model fit is poorest (and way overpredicting).

### Comparing coefficients for both models
```{r compare_coefs}
beta_comparison <- coef_tbl(hmof_partitioned_mod$trained_mod$mod) %>% 
  rename(hmof = beta) %>% 
  left_join(coef_tbl(temp_tob_part_mod$trained_mod$mod), by="bin") %>% 
  rename(tobacco = beta)
beta_comparison %>% 
  gather(cat, beta, tobacco, hmof) %>% 
  overlay_cat_betas(base_hist, ., default_binspec)

# Compare not only betas but also the normalization mean and std
bind_rows(
  t_mean = temp_tob_part_mod$trained_mod$meanz,
  h_mean = hmof_partitioned_mod$trained_mod$meanz,
  t_std = temp_tob_part_mod$trained_mod$stdz,
  h_std = hmof_partitioned_mod$trained_mod$stdz,
  .id = "calc"
  ) %>% 
  gather(bin, value, -calc) %>% 
  spread(calc, value) %>% 
  right_join(beta_comparison, by="bin") %>% 
  rename(t_beta = tobacco, h_beta = hmof) %>% 
  left_join(bin_loc_from_spec(default_binspec), by="bin") %>% 
  arrange(loc) %>% 
  mutate_if(is.numeric, format, digits=2)
# This table shows that there is quite a bit of difference in the standardized mean/std location for the different betas, which could also account for the differences in betas
```

### Training without standardization
Could some of the problems be caused by the z-scoring normalization process?  The distributions of the two databases are different, though the coefficients are overall in the same general probability space (zero to one).  It probably won't work, but it's worth a shot at least, especially given the huge differences in the table above.

```{r no_standardization}
# For now, let's just lightly modify fit_glmnet and friends.  TODO: this can be generalized later if interesting

no_std_fit_glmnet <- function(x, y, lambda = NULL, alpha = 0) {
  # Fits a ridge regression model to a dataframe of predictors, sans normalization
  # The alpha parameter specifies the type of model (ridge=0, LASSO=1, others=elastic net)
  
  orig_x <- x
  # Remove columns with zero std dev
  removed_cols <- x %>% 
    summarize_all(funs(sd)) %>% 
    gather("bin", "sd") %>% 
    filter(sd == 0) %>% # Could also replace with a tolerance
    .$bin
  x <- x[,!(names(x) %in% removed_cols)]

  # Mean-center
  meanz <- x %>% summarize_all(function(a) {0})
  # Thanks to this gem on the mailing list: https://stat.ethz.ch/pipermail/r-help/2006-May/104690.html
  x <- x - meanz[rep(1, times=nrow(x)),]
  
  # Unit variance
  varz <- x %>% summarize_all(function(a) {1})
  x <- x / varz[rep(1, times=nrow(x)),]
  
  # If lambda is not defined, let's calculate the largest (most regularized) value within one SE of the min. cross-validated error
  cvfit <- NULL
  if (is.null(lambda)) {
    trial_lambdas <- 10^seq(10, -2, length = 100)  # Idea from https://www.r-bloggers.com/ridge-regression-and-the-lasso/
    cvfit <- cv.glmnet(as.matrix(x), y, alpha=alpha, nfolds=10, type.measure="mse", lambda=trial_lambdas)
    lambda <- cvfit$lambda.1se
  }
  
  # Actually run the model
  mod <- glmnet(as.matrix(x), y, alpha=alpha, lambda=lambda)  # alpha=0 is ridge regression
  
  # Return the relevant model details
  list(
    mod = mod,
    x = x,
    y = y,
    orig_x = orig_x,
    removed_cols = removed_cols,
    meanz = meanz,
    stdz = varz,
    lambda = lambda,
    alpha = alpha,
    coefs = NULL,
    cv_for_lambda = cvfit
  )
}

no_std_run_fit <- function(x_grid, y_act_with_id, binspec, alpha=0) {
  # Modification of run_bin_model
  x <- x_grid %>% 
    stepped_hist_spec(binspec) %>% 
    spread(key=bin, value=metric)
  y <- x %>% 
    left_join(y_act_with_id, by="id") %>% 
    rename(y = y_act) %>% 
    .$y
  x_id <- x %>% select(id)
  x <- x %>% select(-id)
  
  fitted_mod <- no_std_fit_glmnet(x, y, alpha=alpha)
  tibble(
    fitted_model = list(fitted_mod),
    id_list = list(x_id),
    q2=NA,
    lambda = fitted_mod$lambda, 
    imported_binspec = binspec
  )
}


# Test fitting the hMOF and ToBaCCo separately, but without normalization
no_std_hmof_mod <- no_std_run_fit(
  hmof_hist_sets$training,
  mutate(gcmc_data, y_act=g.L),
  default_binspec
  )$fitted_model[[1]]
gcmc_data %>% 
  mutate(y_act = g.L) %>% 
  eval_test_grid(no_std_hmof_mod, hmof_hist_sets$testing, default_binspec, .)

no_std_tob_mod <- no_std_run_fit(
  tob_hist_sets$training,
  mutate(tobacco_data, y_act=h2.deliv.77),
  default_binspec
  )$fitted_model[[1]]
tobacco_data %>% 
  mutate(y_act = h2.deliv.77) %>% 
  eval_test_grid(no_std_tob_mod, tob_hist_sets$testing, default_binspec, .)

# Now with the hMOF training:
tobacco_data %>% 
  mutate(y_act = h2.deliv.77) %>% 
  eval_test_grid(no_std_hmof_mod, tob_hist_sets$testing, default_binspec, .)
# How does this compare with the original, normalized model?  Nearly identical
tobacco_data %>% 
  mutate(y_act = h2.deliv.77) %>% 
  eval_test_grid(trained_mod, tob_hist_sets$testing, default_binspec, .)
```

Conclusion: the model performance is nearly identical on test data or hMOF to ToBaCCo.  The real difference is that the model interpretation actually becomes clearer with z-scoring: otherwise the model coefficients arrive on different scales, which makes interpretation of the numbers considerably more difficult.


### Diversity sampling
This is the most likely to succeed.  Previous ML studies in the MOF literature have generated the initial training set via maximum diversity selection from all of the parent databases, with a broad range including hMOFs, IZA, and CoRE.

See [caret documentation](http://topepo.github.io/caret/data-splitting.html#splitting-based-on-the-predictors) and other resources on how to perform this sampling.  The diversity algorithms are reviewed in `willett_dissimilarity-based_1999`.  The simplest one (default for caret) is based on an older paper (`kennard_computer_1969`) and was actually used in Cory Simon's Xe/Kr paper (`simon_what_2015`), albeit diversity sampling for each class of nanoporous material.  Note that this is similar to the [DUPLEX algorithm](http://amstat.tandfonline.com/doi/abs/10.1080/00401706.1977.10489581).  The review `borovicka_selecting_2012` is also a great resource for model training strategies.

```{r diversity_sampling_tobacco}
# First, let's begin with only training and testing on ToBaCCo.
# Then we can come back and do something smarter--see how Cory Simon and others handled multiple similar databases

run_model_on_diversity <- function(
  grid_x, y_act_with_id,
  binspec, alpha = 0,
  num_training = 1000, random_init_points = 5  # FIXME: make it 1000 for num_training
  ) {
  # Similar to run_model_on_partitions, but also paritions the data by diversity selection
  # Calculates the ridge or LASSO model on the developed training data, then evaluates the performance on the test data.
  # Returns a partitioned_glmnet object, plus the source data
  # Could also consider changing num_training to use data_split instead, though we'd have to figure out how to subdivide the data into hyperparameter and training.
  
  # Generate training subset.  We have to first scale the relevant columns to ensure the distance calculations are reasonable.
  x <- grid_x %>% 
    stepped_hist_spec(binspec) %>% 
    spread(key = bin, value = metric)
  y <- x %>% 
    left_join(y_act_with_id, by="id") %>% 
    rename(y = y_act) %>% 
    .$y
  x_id <- x %>% select(id)
  x <- x %>% select(-id)
  
  x_orig <- x
  x_std_props <- standardization_from_x(x)
  x <- standardize_from_props(x, x_std_props$removed_cols, x_std_props$meanz, x_std_props$stdz)
  
  ### Now do the diversity section ###
  
  # See caret documentation: http://topepo.github.io/caret/data-splitting.html#splitting-based-on-the-predictors
  set.seed(20171128)
  start_set <- sample(1:nrow(x), random_init_points)
  start_points <- x[start_set,]
  sample_pool <- x[-start_set,]
  dissim_training <- maxDissim(start_points, sample_pool, n=num_training)
  
  # Associate points with the corresponding IDs
  start_ids <- x_id[start_set,]
  pool_ids <- x_id[-start_set,]
  dissim_ids <- pool_ids[dissim_training,]
  
  # Then divide the original grids into the appropriate bins
  partitioned_data <- list(
    training = right_join(grid_x, dissim_ids, by="id"),
    testing = anti_join(grid_x, dissim_ids, by="id")
    )
  
  # Run the model on the diverse sample set
  predictions_with_data <- y_act_with_id %>% 
    mutate(g.L = y_act) %>% 
    run_model_on_partitions(partitioned_data, ., binspec, alpha)
  
  predictions_with_data$partitioned_hists <- partitioned_data
  predictions_with_data$y_orig <- y_act_with_id
  
  predictions_with_data
}

# Find diverse subset for training.  This calculation is expensive.
if (!exists("diverse_results_tob")) {
  diverse_results_tob <- tobacco_data %>% 
    mutate(y_act = h2.deliv.77) %>% 
    run_model_on_diversity(grids_h2, ., default_binspec)
}
print(diverse_results_tob)

# Try to use the ToBaCCo-trained model against the hMOFs
gcmc_data %>% 
  mutate(y_act = g.L) %>% 
  eval_test_grid(diverse_results_tob$trained_mod, hmof_hist_sets$testing, default_binspec, .)

```

### Hybrid hMOF/ToBaCCo model
Let's attempt to generate a model consisting of a random selection of training data from both databases.

```{r hybrid_hmof_tobacco}
# Use 1000 MOFs from each database as a starting baseline, since there's not a particularly rigorous method of assigning how many MOFs we need
# Note: ideally we'd use a maximum diversity method as above, but it's fairly computationally expensive and been shown here not to have too much of an effect on model predictions.

hybrid_gcmc_data <- gcmc_data %>% 
  mutate(id = str_c("h", id)) %>% 
  rename(vf = void.frac) %>% 
  select(id, g.L, vf)
hybrid_tobacco_data <- tobacco_data %>% 
  mutate(g.L = h2.deliv.77) %>% 
  select(id, g.L, vf)
hybrid_data <- bind_rows(hmof = hybrid_gcmc_data, tobacco = hybrid_tobacco_data, .id="database")

set.seed(20171128-1)
hybrid_training_grids <- hmof_h2_grid %>% 
  mutate(id = str_c("h", id)) %>% 
  filter(id %in% sample(unique(.$id), 1000)) %>% 
  bind_rows(filter(grids_h2, id %in% sample(unique(grids_h2$id), 1000)))
hybrid_testing_grids <- hmof_h2_grid %>% 
  mutate(id = str_c("h", id)) %>% 
  bind_rows(grids_h2)
hybrid_grid_sets <- list(training = hybrid_training_grids, testing = hybrid_testing_grids)

hybrid_part_mod <- run_model_on_partitions(hybrid_grid_sets, hybrid_data, default_binspec, 0)
print(hybrid_part_mod)
hybrid_part_mod$pred_df %>% 
  ggplot(aes(y_act, y_pred, color=vf)) +
  geom_point(alpha = 0.6) +
  parity_line +
  facet_wrap(~database) +
  scale_x_continuous(limits = c(0,60)) +
  scale_y_continuous(limits = c(0,60))
```



## CCDC applicability

First, let's test the generality of the hMOF model to predict the CSD MOFs.

```{r ccdc_applicability}
# Derived from notebook 20171106_immediate_followup.Rmd
ccdc_h2_grids <- read_rds("BigData/Robj/ccdc_hist_vals.Rds")
ccdc_gcmc <- read_tsv(
  "BigData/500-CSDmofs.tsv",
  na = c("", "null", "#NAME?", "#VALUE!")
  ) %>% 
  rename(id = MOF, g.L = `deliv g/L`)

ccdc_h2_grids %>% 
  filter(id %in% ccdc_gcmc$id) %>% 
  eval_test_grid(trained_mod, ., default_binspec, mutate(ccdc_gcmc, y_act=g.L))

ccdc_h2_grids %>% 
  filter(id %in% ccdc_gcmc$id) %>% 
  eval_test_grid(hybrid_part_mod$trained_mod, ., default_binspec, mutate(ccdc_gcmc, y_act=g.L))
```

Then, using the model, what are the top CCDC candidates?  Separately, it might also be good to attempt to train a model against the CCDC GCMC data.  What are the coefficients?  Are the predictions for the top candidates the same?

```{r ccdc_predictions}
if (!exists("ccdc_predicted")) {
  ccdc_predicted <- pred_grid(trained_mod, ccdc_h2_grids, default_binspec)
}
ccdc_predicted %>% .$y_pred %>% hist()
paste("Number of MOFs predicted over 40 g/L:", ccdc_predicted %>% filter(y_pred > 40) %>% nrow, "of", nrow(ccdc_predicted))
paste("Over 45 g/L:", ccdc_predicted %>% filter(y_pred > 45) %>% nrow)
paste("Over 10 g/L:", ccdc_predicted %>% filter(y_pred > 10) %>% nrow)
print("Summary of top 1000:")
ccdc_predicted$y_pred %>% sort(decreasing=TRUE) %>% .[1:1000] %>% summary

ccdc_predicted %>% arrange(desc(y_pred)) %>% top_n(1000, y_pred)
```



## Other action items and ideas to explore

### TODO
* Write a first draft of the outline.
* Recalculate predictions/fits with corrected energy grids (sigma = 2.958 Ang instead of 2.598)
* Regenerate plots using traditional textural property descriptors.  Maybe a hybrid model would fix training issues, since it's largely PLD driven?
* Re-implement prediction workflow in C++?

### Data that still needs a final version
* Complete hydrogen adsorption data at 2 bar, 77 K in the ToBaCCo MOFs
* Wilmer's CH4 data to compare against the chemical intutition paper?

### Other possible directions
* Consider generating parity plots, faceted by bin and colored by beta*z (the model contribution).  Do these appear consistent between the hMOFs and ToBaCCo?  Maybe the problem is bin 19 and the difference between a z of 0.5-2.0 (hMOFs) and -1 to 0 (ToBaCCo). Maybe a different bin would help?
* Make a few parity plots from the Q2 analysis.  Is the good prediction consistent across histogram parameters?  How about for ToBaCCo, since it's likely more sensitive?
* Trying out traditional textural properties, or adding them to my set of energy-based features
* Would it be helpful to run PCA on the hMOF and/or ToBaCCo feature matricies?  Maybe the scores would show clustering between good/bad performance?

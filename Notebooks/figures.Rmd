---
title: "Generating figures for the manuscript"
author: "Ben Bucior"
date: "December 20, 2017"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r setup_data, include=FALSE}
# Has to be run in a separate block as above
source("Notebooks/setup_data.R")
library(plotly)
```

```{r setup_figs}
# Setup figure parameters
library(cowplot)
# Per [RSC guidelines](http://www.rsc.org/journals-books-databases/journal-authors-reviewers/submit-your-article/#figures-graphics-images),
# figures should be single column (8.3cm) or double column (17.1cm) and no longer than 23.3cm.
# For final submission, they like 600 dpi TIFF files.  Before then, let's use smaller figs.
default_dpi <- 300
theme_set(theme_cowplot(font_size = 10))
# Example plot that works well:
# save_plot("test.png", hmof_partitioned_mod$plots$parity_full + xlab("Actual GCMC uptake") + ylab("Predicted uptake"), base_width=8.3/2.54, base_height=8.3/2.54/1.1, dpi=300)
save_ben_fig <- function(p, filename, ...) {
  save_plot(
    filename,
    p,
    base_width = 8.3 / 2.54,  # RSC width in inches
    base_height = NULL,  # calculate from base_aspect_ratio and the width
    dpi = default_dpi,
    ...
    )
}
# Better example:
# hmof_partitioned_mod$plots %$% plot_grid(parity_training, parity_testing, ncol=1, labels=paste0("(", letters, ")")) %>% save_ben_fig("test.png", nrow = 2)
# See also [vignette](https://cran.r-project.org/web/packages/cowplot/vignettes/plot_grid.html)
```

## Figures for manuscript: main text

Recreate (and export) figures in one block.  Possibly will be externalized into a separate script later.

```{r make_figures}
# Figures on adsorption strength (conceptual) and ML workflow (to update, but external)
# (likely generated within other software)

# Coefficients for ridge regression model
hmof_partitioned_mod <- run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, default_binspec, 0)
trained_mod <- hmof_partitioned_mod$trained_mod
base_hist <- hmof_h2_grid %>% 
  filter(id == 71) %>% 
  plot_hist_bins(default_binspec)
base_hist %>% overlay_cat_betas(coef_tbl(trained_mod$mod), default_binspec)

# Beta coefficients for different pressures and temperatures
# For help on NSE in dplyr, see vignette("programming")
library(rlang)  # see also https://github.com/tidyverse/rlang/issues/116
mutate_col_to_gL <- function(df, newcol) {
  mutate(df, g.L = !!sym(newcol))
}
## Previously used for the CH4 plots, but no reason not to adapt it for T and P of H2
list(`100 bar` = "va100bar", `2 bar` = "va2bar") %>% 
  map(mutate_col_to_gL, df = gcmc_data) %>%
  map(function(x) run_model_on_partitions(hmof_hist_sets, x, default_binspec, 0)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec)
p_2bar_grids <- hmof_h2_grid %>% filter(id %in% p_2bar_data$id)
p_2bar_sets <- partition_data_subsets(p_2bar_grids, p_2bar_data, DATA_SPLIT)
list(`160 K` = "h2.g.L.2.160", `77 K` = "va2bar") %>% 
  map(mutate_col_to_gL, df = p_2bar_data) %>%
  map(function(x) run_model_on_partitions(p_2bar_sets, x, default_binspec, 0)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec)


# Model fit for hMOF ridge regression mdoel
print(hmof_partitioned_mod)
# Print stats for fitting the ToBaCCo data to itself
# tob_y_to_join and tob_hist_sets are defined in setup_data.R
run_model_on_partitions(tob_hist_sets, tob_y_to_join, default_binspec, 0)
# ...and using the hMOF-trained model directly on the ToBaCCo database
tobacco_data %>% 
  mutate(y_act = fh.h2.g.L.100.77 - fh.h2.g.L.2.77) %>% 
  eval_test_grid(hmof_partitioned_mod$trained_mod, tob_hist_sets$testing, default_binspec, .)

# Robustness to histogram binning strategy
hyper_tuned_hist <- read_rds("BigData/Robj/hyper_tuned.Rds")
plot_robustness_bin_gap <- 0.05
plot_robustness_beta_max <- 5
hyper_tuned_hist %>% 
  mutate(beta = ifelse(beta > plot_robustness_beta_max, plot_robustness_beta_max, beta)) %>% 
  mutate(beta = ifelse(beta < -plot_robustness_beta_max, -plot_robustness_beta_max, beta)) %>% 
  mutate(lower = lower + plot_robustness_bin_gap, upper = upper - plot_robustness_bin_gap) %>% 
  ggplot(aes(
    x = binwidth,
    ymin = lower,
    ymax = upper,
    color = beta
  )) +
  geom_linerange(size=2) +
  coord_flip() +
  scale_color_gradientn(
    colors = c("red", "darkgray", "blue"),
    limits = c(-plot_robustness_beta_max, plot_robustness_beta_max),
    breaks = c(-plot_robustness_beta_max, 0, plot_robustness_beta_max),
    labels = c(paste0(-plot_robustness_beta_max,"-"), "0", paste0(plot_robustness_beta_max,"+")),
    name=expression(beta[ridge])
    ) +
  labs(
    y = "Energy (kJ/mol)",
    x = "Width of histogram bins (kJ/mol)"
  )

# Models based on textural properties
# TODO: add density to the MOFs.  We could also consider DPD, though that's only based on PSD
# TODO: all of the "good" models so far have been based on methane.  Can we reproduce the good predictivity for CH4 using MLR?  Maybe the problem is like hydrogen storage, where high P is predicted by Chahine's rule
# Serves as a baseline for how well the energy histogram is working.
# Selected properties based on a quick literature review of ML:
# We should use rho, VF, VSA/GSA, and potentially LCD or other pore diameters.
# Cory Simon, EES 2015, 1190-1199: Density, VF, largest included sphere, AVSA
# Fernandez, JPCC 2013, 7681-7689: DPD, VF, GSA (and VSA)
# Pardakhti, ACS Comb Sci 2017, 7681-7689: VF, GSA, density, DPD, LCD, Interp. cap, Cat nets
# Plus chemical properties: element counts, metal type, degree of saturation, metallic %, ratios of atom types
# Thornton, Chem Mater 2017, 2844-2854: VF, density, adsorption energy (kJ/mol), pore diameter, GSA, VSA
# First, prepare the input data
textural_props <- 
  gcmc_data %>% 
  mutate(h2.deliv.77 = va100bar - va2bar) %>% 
  select(id, vf=void.frac, lcd, gsa=surf.area..mass., vsa=surf.area..vol., h2.deliv.77) %>% 
  mutate(id = str_c("h", id))
textural_props <- 
  tobacco_data %>% 
  mutate(h2.deliv.77 = fh.h2.g.L.100.77 - fh.h2.g.L.2.77) %>% 
  select(id, vf, lcd, gsa, vsa, h2.deliv.77) %>% 
  bind_rows(hMOF = textural_props, ToBaCCo = ., .id = "db") %>% 
  na.omit
# Run ridge regression models and generate parity plots in the same style as the energy grid method
textural_hmofs <- 
  textural_props %>% 
  filter(db == "hMOF") %>% 
  select(-db)
textural_training_rows <- sample(nrow(textural_hmofs), 1000)
textural_mod <- 
  textural_hmofs %>% 
  .[textural_training_rows,] %>% 
  select(-id) %>% 
  {fit_glmnet(select(., -h2.deliv.77), .$h2.deliv.77)}
# (will probably be the same fit_glmnet function)
textural_pred <- 
  textural_hmofs %>% 
  .[-textural_training_rows,] %>% 
  select(-id, -h2.deliv.77) %>% 
  pred_glmnet(textural_mod, .)
textural_act <- textural_hmofs[-textural_training_rows, "h2.deliv.77"]
parity_plot(textural_act, textural_pred, alpha=0.1)
postResample(pred=textural_pred, obs=textural_act)
textural_mod$lambda


# Methane predictions
p_ch4_vol <- p_2bar_data %>% 
  mutate(g.L = ch4.cm3.cm3.65.298 - ch4.cm3.cm3.5_8.298) %>% 
  run_model_on_partitions(p_ch4_sets, ., ch4_binspec, 0)
p_ch4_vol$plots$parity_full +
  scale_x_continuous(limits = c(0,250)) +
  scale_y_continuous(limits = c(0,250))
ch4_base_hist <- p_ch4_grids %>% 
  filter(dirname == "hMOF-71") %>%  # Using the same MOF as the sample H2 histogram
  plot_hist_bins(ch4_binspec)
list(`5.8 bar` = "ch4.cm3.cm3.5_8.298", `65 bar` = "ch4.cm3.cm3.65.298") %>% 
  map(mutate_col_to_gL, df=p_2bar_data) %>%
  map(function(x) run_model_on_partitions(p_ch4_sets, x, ch4_binspec, 0)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(ch4_base_hist, ., ch4_binspec, scaling = 20.0)

# CCDC applicability, etc.
print("Note: revised below in another code block")
ccdc_h2_grids %>% 
  filter(id %in% ccdc_gcmc$id) %>% 
  eval_test_grid(trained_mod, ., default_binspec, mutate(ccdc_gcmc, y_act=g.L))
```


## Supporting Information for manuscript

This section is still largely TODO but at least can be done in part for some of the more important figures.

### Outline
Before working on SI revisions, I should take a step back to organize what content now belongs there.  Until then, here's a few new (random) figures I thought about that will be important for the (SI) story.

```{r unsorted_si_figures}
# In general, there should be an inverse relationship between void fraction and the infinite bin.
# The match will not be exact because the energy cutoffs are rather different.
grids_h2 %>% 
  stepped_hist_spec(default_binspec) %>% 
  filter(bin == "Inf") %>% 
  rename(`Inf bin` = metric) %>% 
  left_join(tobacco_data, by="id") %>%
  ggplot(aes(vf, `Inf bin`)) +
  geom_point(alpha=0.5)

# Geometric property distributions for the databases
textural_props %>% 
  select(-id) %>%
  rename(
    `Gravimetric\nsurface area` = gsa,
    `H2 deliverable\ncapacity (g/L)` = h2.deliv.77,
    `Largest cavity\ndiameter` = lcd,
    `Void fraction` = vf,
    `Volumetric\nsurface area` = vsa
    ) %>% 
  gather("property", "value", -db) %>% 
  ggplot(aes(value)) +  # Trying to use ..density.. here results in strange behavior
  geom_histogram(bins=30) +
  coord_cartesian() +
  facet_grid(db ~ property, scales = "free") +
  background_grid()

```



## Preparing CSD MOFs for screening
In order to successfully analyze the CSD MOFs, we need to filter out the purely metallic structures.  Those are only pseudo-materials of specially spatially-arranged atoms.  Since they aren't MOFs, the model does not predict them well, either.

Finding MOFs without carbon isn't too difficult.  Open Babel has a convenient [command line option](https://openbabel.org/docs/dev/Command-line_tools/babel.html#append-option) to print out the molecular formula.  Then, we can use some basic text processing tools within R to explore which structures contain carbon, oxygen, etc.

```
cd /cygdrive/c/Users/Benjamin/Git/EnergyGrid/BigData/cifs_from_csddata;
obabel *.cif -ab -otxt --append "\tFORMULA" >> ../csd_formula.txt
# Note, the output has multiple warnings about atom labels like '0', '1', and '1.'.
```

```{r csd_formula, eval=FALSE, include=FALSE}
# Temporarily disable this (expensive) section until we're ready to do the analysis.
# It might actually belong in a separate notebook file if influencing other steps in the analysis procedure.
raw_csd_formulas <- read_tsv(
  "BigData/csd_formula.txt",
  col_names = c("cif", "str_formula"),
  col_types = "cc"
  )
library(CHNOSZ)

# We can convert a purrr list to data.frame using dplyr::bind_rows.
# See also https://github.com/tidyverse/dplyr/issues/1676 for the as.list invocation,
# which converts each named vector into its own list
# TODO: consider using `safely` to parse or hide errors
csd_formulas <- 
  raw_csd_formulas$str_formula %>% 
  as.list %>% 
  map(makeup) %>% 
  map(as.list) %>% 
  bind_rows %>% 
  bind_cols(raw_csd_formulas, .)

# Let's implement a less stringent form of the CoRE MOF requirements.
# Here, we'll require MOFs to contain at least one of C, N, P, and S.
# First, how different are the results if we filter by has_"just C" instead of the whole set?
csd_formulas %>% filter(is.na(C) & is.na(N) & is.na(P) & is.na(S)) %>% nrow
csd_formulas %>% filter(is.na(C)) %>% nrow
# Based on that analysis, it's probably okay for a first pass to be less restrictive
print("Structures to remove (without C, N, P, or S):")
csd_formulas <- 
  csd_formulas %>% 
  mutate(missing_cnps = (is.na(C) & is.na(N) & is.na(P) & is.na(S)))
no_cnps <- csd_formulas %>%
  filter(missing_cnps) %>% 
  select(cif, str_formula)
no_cnps
no_cnps %>% 
  select(cif) %>% 
  write_tsv("BigData/Output/csd_mofs_without_cnps.txt", col_names = FALSE)

# Now to look at the CCDC GCMC results and predictions without the pure metals
ccdc_h2_grids %>% 
  filter(id %in% ccdc_gcmc$id) %>% 
  filter(!(id %in% no_cnps$cif)) %>% 
  eval_test_grid(trained_mod, ., default_binspec, mutate(ccdc_gcmc, y_act=g.L))

if (!exists("ccdc_predicted")) {
  ccdc_predicted <- pred_grid(trained_mod, ccdc_h2_grids, default_binspec)
}
ccdc_predicted %>% .$y_pred %>% hist()
print("Summary of top 1000:")
ccdc_predicted %>% 
  filter(!(id %in% no_cnps$cif)) %>% 
  .$y_pred %>%
  sort(decreasing=TRUE) %>% .[1:1000] %>%
  summary

csd_formulas %>% 
  filter(!missing_cnps) %>% 
  select(cif, str_formula) %>% 
  rename(id = cif) %>% 
  inner_join(ccdc_predicted, by="id") %>% 
  arrange(desc(y_pred)) %>% top_n(1000, y_pred)
# Based on looking at a few of the top structures, the cleaned up versions are 1D rods, so not likely synthesizable.  But that's more a flaw of the database than the screening method, though it's possible the prediction method may not work well for non-MOF structures.
```



## TODO notes from meetings
Different things to consider for the analysis and manuscript.

### General
* Finish a second revision of the outline/manuscript
* Aggregate TODOs into this document to wrap up exploration and decisions
* Organize figures for SI and clean that up
* Run grids for CCDC MOFs
* Compile RASPA grid data for CH4
* Send Scotty list of top CCDC candidates once we have a (near-final?) model
* Calculate density for the hMOFs (and consider recalculating all of the textural properties in Zeo++)
* See base case analysis above for plain textural properties.  Understand the limitations of MLR on CH4 (vs. H2).

### ToBaCCo errors
* Two separate models for high P and low P?  Might be easier for the model to understand the physics. (have I tried a parity plot of the combined model of separate?  That may be an interesting first step).
* Combining the 1000 + 1000 might be the way to go for the paper, since it also simplifies the story.

### Model formulation
* Check if glmnet's standardize flag is causing the issues with CV.
* Skip all standardization AND set the intercept to zero??
* Recalculate Q2 once we relax the penalty term to be lower.  Beyond RMSE, etc., how does Q2 vary for the models
* Do we need standardization?  If not, consider that it would be easier (for interpretability) not to normalize, and save the justification in the SI.

### Figures 
* Set up better infrastructure for automating figure generation
* Label Q2 (and other stats?) directly on the plot.
* Set the size of the figures within R to get consistent font sizes and typefaces. (easiest way would probably measuring the Latex column width in inches, and setting the figure DPI.  Maybe replace png with PDF, too)
* "training on a combined database" should use independent data for training and testing
* Beta plot: consider drawing the background as the average histogram of the data.  Betas could be drawn as hlines instead of dots.
* Consider drawing in [cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html)
* Ranking plots: rank from 1 to 500 with inverted axes (top right corner is (1,1) for ranking)
* Fix aspect ratio within R notebook

### Other from group meeting 12/7
* What about higher T for desorption?
* How do the range of the hMOF and ToBaCCo energies compare?  Is the maximally attractive ToBaCCo region significantly lower than the hMOFs? (related: does our ToBaCCo energy go low enough?)
* Can we optimize uptake for a given bin (calculating a maximum adsorption for an artificial pseudo-MOF using the histogram bins?)

### Code
* Place standardization as a flag for model fitting instead of repeated functions like `no_std_fit_glmnet` in the `no_standardization` code block

#### Other possible directions
* What sort of capacity could you get with a uniform background potential? (`12_6_2_0` potential against a single MOF pseudo atom in a huge box?  Of course no tail corrections or shifting the potential (or 12/6 LJ params)).  Of course, validate against a bias of 0 kJ/mol to verify that the "deliverable capacity" is just the difference in 100 bar and 2 bar densities. (in curiosity, how closely does the "density" of the pseudobox match the beta coefficient for each raw bin?)
* Make a few parity plots from the Q2 analysis.  Is the good prediction consistent across histogram parameters?  How about for ToBaCCo, since it's likely more sensitive?
* Would it be helpful to run PCA on the hMOF and/or ToBaCCo feature matricies?  Maybe the scores would show clustering between good/bad performance?
* Is there a relationship between the betas at different P/T?  That could help optimize operating parameters if we can interpolate predictive models.
* Use Wilmer's CH4 data to compare against the chemical intutition paper?


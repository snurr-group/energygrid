---
title: "Generating figures for the manuscript"
author: "Ben Bucior"
date: "December 20, 2017"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r setup_data, include=FALSE}
# Has to be run in a separate block as above
source("Notebooks/setup_data.R")
library(plotly)
library(jsonlite)  # for exporting info on the training data used
```

```{r setup_figs}
# Setup figure parameters
library(cowplot)
# See [intro documentation](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html) and [vignette](https://cran.r-project.org/web/packages/cowplot/vignettes/plot_grid.html)
# Per [RSC guidelines](http://www.rsc.org/journals-books-databases/journal-authors-reviewers/submit-your-article/#figures-graphics-images),
# figures should be single column (8.3cm) or double column (17.1cm) and no longer than 23.3cm.
# For final submission, they like 600 dpi TIFF files.  Before then, let's use smaller figs.
default_dpi <- 300
theme_set(theme_cowplot(font_size = 10))
default_fig_dir <- "Figs"
default_subfig_labels <- paste0("(", letters, ")")

mkdirs(default_fig_dir)
save_ben_fig <- function(p, filename, parent_dir = default_fig_dir, ...) {
  save_plot(
    file.path(parent_dir, filename),
    p,
    base_width = 8.3 / 2.54,  # RSC width in inches
    base_height = NULL,  # calculate from base_aspect_ratio and the width
    dpi = default_dpi,
    ...
    )
}
make_draft <- function(p, msg="DRAFT!", size=72) {
  # Derived from cowplot intro
  ggdraw(p) +
    draw_label(msg, angle = 45, size = size, alpha = .2)
}
save_si_fig <- function(p, filename, parent_dir = default_fig_dir, bheight = 3.3, bwidth = NULL, ...) {
  save_plot(
    file.path(parent_dir, filename),
    p,
    base_width = bwidth,  # defaults to NULL (automatic based on the aspect ratio)
    base_height = bheight,  # Height in inches
    dpi = default_dpi,
    ...
    )
}

# Example:
# hmof_partitioned_mod$plots %$% plot_grid(parity_training, make_draft(parity_testing), ncol=1, labels=default_subfig_labels, label_size=12) %>% save_ben_fig("test.png", nrow = 2)

# LaTeX export
save_fig_table <- function(df, filename, col_header=FALSE, parent_dir=default_fig_dir) {
  # Writes a table to a Latex file which can be read with \input
  # https://www.r-bloggers.com/exporting-r-tables-in-latex/
  append_data <- FALSE
  if (is.character(col_header) & length(col_header) == 1) {
    cat(col_header, file = file.path(parent_dir, filename))
    col_header <- FALSE
    append_data <- TRUE
  }
  df %>% 
    as.data.frame() %>%  # tibbles add a strange header to the file
    write.table(
      file.path(parent_dir, filename),
      quote = FALSE,
      row.names = FALSE,
      col.names = col_header,
      eol="\\\\\n",
      sep=" & ",
      append = append_data
      )
}

save_fig_table_with_colnames <- function(df, filename, ...) {
  save_fig_table(
    df,
    filename,
    col_header=paste0(paste(colnames(df), collapse = " & "), "\\\\\n\\hline\n"),
    ...
    )
}

format_column_digits <- function(df, cols_vec, ndigit = 2) {
  # Formats specified cols (arg is a character vector of names) with the correct number of digits
  # Call this function as part of a pipeline to format a table before trying to save it.
  # Set ndigit to zero to print a column of doubles as an integer
  mutate_at(df, cols_vec, function(x) format(x, digits=ndigit, nsmall=ndigit))
}
```

## Figures for manuscript: main text

Recreate (and export) figures in a few blocks.  Possibly will be externalized into a separate script later.

```{r prep_conceptual_figures}
# Figures on adsorption strength (conceptual) and ML workflow (to update, but external pptx in the Box Projects folder)
p_concept_isotherms <-
  list("hMOF-1898", "hMOF-5072244") %>% 
  set_names(., .) %>% 
  map_chr(~paste0("BigData/Emails/isotherm-a-b-20180210/hydrogen_77_", .x, ".dat")) %>% 
  map_dfr(
    ~ read_tsv(.x, skip=2, col_types="d--d----", col_names=c("Pa", "cm3.cm3")),
    .id = "id"
  ) %>% 
  mutate(g.L = cm3.cm3 * 2.0 / 22.4, bar = Pa / 1e5)
p_concept_isotherms %>% 
  ggplot(aes(bar, g.L, color=id)) +
  geom_smooth(se=FALSE)

# Add a fit to a Type I Langmuir isotherm, based on Arun's data (see PDF in data directory)
cartoon_pressures <- c(seq(0.05, 0.95, 0.05), 1:100)
double_arrow <- arrow(ends="both", type="closed", length=unit(1.5, "mm"))
p_sub_isotherm <- 
  list(
    `hMOF-1898` = c(2.62529e-5, 4.0579e-7) * 1e5,  # convert fit from Pa to bar
    # also fix error in spreadsheet converting from cm3/cm3 to g/L (mult. vs. division)
    `hMOF-5072244` = c(0.05656/ 11.2^2, 9.43599E-6) * 1e5
  ) %>% 
  map_dfr(~ data_frame(bar = cartoon_pressures, A = .x[1], B = .x[2]), .id = "id") %>% 
  mutate(g.L = A*bar / (1 + B * bar)) %>% 
  ggplot(aes(bar, g.L, color = id, shape = id)) +
  geom_line() +
  #geom_point(data = p_concept_isotherms) +
  xlab("Pressure (bar)") + ylab("Uptake (g/L)") +
  geom_vline(xintercept = c(2, 100), linetype="dotted") +
  scale_color_manual(values = c(2, 4), breaks = c("hMOF-1898", "hMOF-5072244"), guide = FALSE) +
  scale_shape_discrete(guide=FALSE) +
  scale_x_log10(breaks=c(2, 100), limits=c(5e-2, 135)) +
  annotate("segment", x = 120, xend = 120, y = 4.9, yend = 51.9, color = 2, arrow = double_arrow) +
  annotate("segment", x = 1.5, xend = 1.5, y = 31.2, yend = 47.3, color = 4, arrow = double_arrow) +
  annotate("text", 92, 6, label = "47 g/L\nMore delivery", color = 2, hjust = 1, vjust = 0) +
  annotate("text", 1.2, 31.2, label = "16 g/L\nSteep isotherm", color = 4, hjust = 1, vjust = 0) +
  annotate("segment", x = 2, xend = 100, y = 4.9, yend = 4.9, color = 2, linetype = "dotted") +
  annotate("segment", x = 2, xend = 100, y = 47.3, yend = 47.3, color = 4, linetype = "dotted")

# Import ad-hoc grids
ad_hoc_h2_grids <- read_rds("BigData/Robj/adhoc_h2.Rds")

p_sub_energies <- 
  ad_hoc_h2_grids %>%
  mutate(id = str_sub(id, 4)) %>% filter(id %in% c("hMOF-1898", "hMOF-5072244")) %>%
  mutate(loc = lower + upper) %>% 
  group_by(id) %>% mutate(dens = counts / sum(counts)) %>% ungroup() %>% 
  ggplot(aes(loc, dens, color = id, shape = id)) + geom_point() +
  xlim(c(-15, 0)) + ylim(c(0, 0.15)) +
  xlab("Energy (kJ/mol)") + ylab("Fraction of UC") +
  scale_color_manual(values = c(2, 4), breaks = c("hMOF-1898", "hMOF-5072244")) +
  theme(legend.position=c(0, 0.8), legend.title = element_blank()) +
  annotate("text", -15, 0.05, label=as.character(expression("Adsorbs H"[2]~"too strongly")), parse = TRUE, color = 4, hjust=0)  # need as.character: https://stackoverflow.com/questions/27303019/ggplot-annotate-with-greek-symbol-and-1-apostrophe-or-2-in-between-text

plot_grid(
  p_sub_energies,
  p_sub_isotherm,
  ncol=1,
  align = "v",
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>%
  save_ben_fig("binding_cases.png", nrow = 2, base_aspect_ratio = 2.0)
# Might also consider the [patchwork library](https://github.com/thomasp85/patchwork) for future ggplot work


# Resources for schematic figure in pptx
# loop_cif in c(55, 2000189, 5072190)
# Run on each, then save to the pptx
for (hid in c(55, 2000189, 5072190)) {
  (ad_hoc_h2_grids %>% 
    filter(id == paste0("hMOF-", hid)) %>% 
    plot_hist_bins(default_binspec) +
    scale_y_continuous() +  # restore y axis line
    coord_cartesian(ylim=c(0,0.50), xlim=c(-8.5, 0.50)) +
    theme_diagram_min +
    theme(aspect.ratio = 1.0)
    ) %>% print
}

```

```{r make_figures}
# Coefficients for ridge regression model
hmof_partitioned_mod <- run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, default_binspec, db_name = "hMOFs")
mixed_partitioned_mod <- run_model_on_partitions(mixed_h2_hist_sets, mixed_h2_y_to_join, default_binspec, db_name = "mixed")
#trained_mod <- hmof_partitioned_mod$trained_mod  # Overwrite this with the mixed model immediately below
#trained_mod <- mixed_partitioned_mod$trained_mod  # Note: removing the definition of `trained_mod` so that the plots are more explicit about which model is being selected

base_grid <- filter(ad_hoc_h2_grids, id == "hMOF-0")
base_hist <- base_grid %>% plot_hist_bins(default_binspec)
p_beta_orig <- base_hist %>%
  overlay_cat_betas(coef_tbl(hmof_partitioned_mod$trained_mod$mod), default_binspec) %>% 
  replot_hist_and_beta(undo_scaling = BETA_H2_SCALING)
p_beta_orig

# Beta coefficients for different pressures and temperatures
# For help on NSE in dplyr, see vignette("programming")
library(rlang)  # see also https://github.com/tidyverse/rlang/issues/116
mutate_col_to_gL <- function(df, newcol) {
  mutate(df, g.L = !!sym(newcol))
}
## Previously used for the CH4 plots, but no reason not to adapt it for T and P of H2
p_beta_diff_p <- 
  list(`100 bar` = "va100bar", `2 bar` = "va2bar") %>% 
  map(mutate_col_to_gL, df = gcmc_data) %>%
  map(function(x) run_model_on_partitions(hmof_hist_sets, x, default_binspec)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec) %>% 
  replot_hist_and_beta(undo_scaling = BETA_H2_SCALING)

p_2bar_grids <- hmof_h2_grid_full %>% filter(id %in% p_2bar_data$id)
p_2bar_grids$id %>% unique %>% write_rds("BigData/Output/p_2bar_grids_id.Rds")

# Filter out these 2bar data again to be 1250 at most
simplified_2bar_training_ids <- archived_training_ids$`hMOF separate conditions`$training
simplified_2bar_testing_ids <- archived_training_ids$`hMOF separate conditions`$testing
simplified_2bar_testing_ids <- sample(simplified_2bar_testing_ids, 2250-length(simplified_2bar_training_ids))
p_2bar_grids <- p_2bar_grids %>% filter(id %in% c(simplified_2bar_training_ids, simplified_2bar_testing_ids))


hmof_hist_sets <- partition_data_subsets(hmof_h2_grid, hmof_y_to_join, DATA_SPLIT, archived_training_ids$`hMOF H2`$training)

p_2bar_sets <- partition_data_subsets(p_2bar_grids, p_2bar_data, DATA_SPLIT, archived_training_ids$`hMOF separate conditions`$training)

p_160k_grids <- read_rds("BigData/Robj/hmof_160k.Rds") %>% 
  mutate(id = as.integer(str_sub(id, 6)))  # remove the "hMOF-" header

p_beta_diff_t <- 
  filter(p_160k_grids, id %in% p_2bar_sets$training$id) %>% 
  {list(
    `160 K` = run_bin_model_spec(., mutate(p_2bar_data, g.L = h2.g.L.2.160), default_binspec),
    `77 K` = run_bin_model_spec(p_2bar_sets$training, mutate(p_2bar_data, g.L = va2bar), default_binspec)
    )} %>%
  map(function(x) x$fitted_model[[1]]$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec) %>% 
  replot_hist_and_beta(undo_scaling = BETA_H2_SCALING)
# Generate the plots
plot_grid(
  p_beta_orig,
  p_beta_diff_p + theme(legend.position=c(0.15, 0.15), legend.direction="horizontal") + coord_cartesian(ylim=c(-100, 250)),  # Extend limits to give some breathing room for the legend
  p_beta_diff_t + theme(legend.position=c(0.15, 0.15), legend.direction="horizontal"),
  ncol=1,
  align = "v",
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>%
  save_ben_fig("betas.png", nrow = 3, base_aspect_ratio = 2)


# Model fit for hMOF ridge regression model
print(hmof_partitioned_mod)
hmof_partitioned_mod$plots %$%
  plot_grid(
    parity_training,
    parity_testing,
    ncol = 1,
    labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
    ) %>% 
  save_ben_fig("parity.png", nrow = 2)

# Print stats for other databases, including fitting the ToBaCCo data to itself
# tob_y_to_join and tob_hist_sets are defined in setup_data.R
p_tob_mod <- run_model_on_partitions(tob_hist_sets, tob_y_to_join, default_binspec, db_name = "ToBaCCo MOFs")
p_tob_mod
# ...and using the hMOF-trained model directly on the ToBaCCo database
p_tob_from_hmof <- 
  tobacco_data %>% 
  mutate(y_act = fh.h2.g.L.100.77 - fh.h2.g.L.2.77) %>% 
  eval_test_grid(hmof_partitioned_mod$trained_mod, tob_hist_sets$testing, default_binspec, ., db_name = "ToBaCCo MOFs\n(hMOF model)")

# CCDC applicability, etc.
no_cnps <- read_tsv("BigData/Robj/csd_mofs_without_cnps.txt", skip=0, col_names="id", col_types="c")$id
print(paste("Number of CCDC MOFs without CNPS:", length(no_cnps)))
p_ccdc_random <- 
  ccdc_h2_grids %>% 
  filter(id %in% ccdc_gcmc$id) %>% 
  #mutate(has_cnps = !(id %in% no_cnps)) %>% 
  #filter(!(id %in% no_cnps)) %>% 
  eval_test_grid(mixed_partitioned_mod$trained_mod, ., default_binspec, mutate(ccdc_gcmc, y_act=g.L), db_name = "CCDC MOFs\n(mixed model)")
p_ccdc_parity <- 
  p_ccdc_random$plots$parity_testing

# Trying the mixed model to predict the ToBACCo MOFs
tob_from_mixed <- 
  tobacco_data %>%
  mutate(y_act = fh.h2.g.L.100.77 - fh.h2.g.L.2.77) %>%
  eval_test_grid(mixed_partitioned_mod$trained_mod, tob_hist_sets$testing, default_binspec, ., db_name = "ToBaCCo MOFs\n(mixed model)")
# And the hMOFs for good measure
hmof_from_mixed <- 
  hmof_y_to_join %>% 
  mutate(y_act = g.L) %>% 
  eval_test_grid(mixed_partitioned_mod$trained_mod, hmof_hist_sets$testing, default_binspec, ., db_name = "hMOFs\n(mixed model)")

# Generate the (large) figure for model generality
p_tob_mod$plots %$%
  plot_grid(
    parity_testing,
    #p_tob_from_hmof$plots$parity_testing,
    p_tob_from_hmof$pred_df %>%  
      rename(LCD = lcd) %>% # adapted from $plots$parity_testing and parity_plot
      arrange(LCD) %>% 
      {ggplot(., aes(y_act, y_pred, color=LCD)) +
        geom_point() +
        xlab("'Actual' capacity (GCMC, g/L)") +
        ylab("Predicted capacity (LASSO, g/L)") +
        expand_limits(x = 0, y = 0) +
        scale_x_continuous(limits = c(0,60)) +
        scale_y_continuous(limits = c(0,60)) +
        coord_fixed() +
        parity_line +
        theme(legend.position=c(0.82, 0.47)) +
        scale_color_gradient(low="gray", high="purple4")
        } %>% 
      annotate_plot(paste0("Testing data\n", nrow(p_tob_from_hmof$pred_df), " ToBaCCo MOFs\n(hMOF model)"), "top.left", "#0070C0") %>% 
      annotate_plot(paste0(
        #"R\u00B2 = ", format(p_tob_from_hmof$testing_fit["Rsquared"], digits=2), "\n",
        "MAE = ", format(p_tob_from_hmof$testing_fit["MAE"], digits=2, nsmall=1), " ", "g/L", "\n",
        "RMSE = ", format(p_tob_from_hmof$testing_fit["RMSE"], digits=2, nsmall=1), " ", "g/L"
        ), "bottom.right"),
    tob_from_mixed$plots$parity_testing,
    p_ccdc_parity,
    ncol = 2,
    #labels=default_subfig_labels, label_size=12
    labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
    ) %>% 
  save_ben_fig("general_parity.png", nrow = 2, ncol = 2)



# Robustness to histogram binning strategy
hyper_tuned_hist <- read_rds("BigData/Robj/hyper_tuned.Rds")  # FIXME: This will need to be regenerated
plot_robustness_bin_gap <- 0.05
plot_robustness_beta_max <- 0.5 * BETA_H2_SCALING / 5.0  # match the parameters on the beta plots, scaled back a little bit
hyper_tuned_labels <- 
  hyper_tuned_hist %>% 
  filter(upper==Inf) %>% 
  gather(labeltype, labelval, c("q2", "MAE")) %>% 
  mutate(binwidth = binwidth + 0.10) %>% # shift the labels slightly
  mutate(labelval = format(labelval, digits=2, nsmall=2)) %>%  # nsmall specifies the number of digits after the decimal point
  mutate(labelval = ifelse(near(binwidth, 2.5+0.10), paste(ifelse(labeltype=="q2", "Q\u00B2", labeltype), "=", labelval), labelval)) %>% 
  mutate(lower = ifelse((labeltype=="q2"), -6, lower))
p_robust <- 
  hyper_tuned_hist %>% 
  mutate(beta = ifelse(beta > plot_robustness_beta_max, plot_robustness_beta_max, beta)) %>% 
  mutate(beta = ifelse(beta < -plot_robustness_beta_max, -plot_robustness_beta_max, beta)) %>% 
  mutate(lower = lower + plot_robustness_bin_gap, upper = upper - plot_robustness_bin_gap) %>% 
  ggplot(aes(
    x = binwidth,
    ymin = lower,
    ymax = upper,
    color = beta
  )) +
  geom_linerange(size=2) +
  coord_flip() +
  scale_color_gradientn(
    colors = c("red", "lightgray", "blue"),
    limits = c(-plot_robustness_beta_max, plot_robustness_beta_max),
    breaks = c(-plot_robustness_beta_max, 0, plot_robustness_beta_max),
    labels = c(paste("<",-plot_robustness_beta_max), "0", paste(">",plot_robustness_beta_max)),
    name=expression(beta[LASSO])
    ) +
  scale_x_continuous(breaks=(0:5)/2.0, limits=c(0, NA)) +
  labs(
    y = "Energy (kJ/mol)",
    x = "Width of histogram bins (kJ/mol)"
  ) +
  #ylim(c(-12.5, 2.5))
  geom_text(data=hyper_tuned_labels, aes(x=binwidth, y=lower, label=labelval), color="black", size=3, hjust=1) +
  # add training data/fit to the plot
  theme(legend.key.width = unit(2.5, "mm"))
save_ben_fig(p_robust, "beta_robustness.png", base_aspect_ratio=1.3)
p_robust


# Methane predictions
p_ch4_vol <- p_2bar_data %>% 
  mutate(g.L = ch4.cm3.cm3.65.298 - ch4.cm3.cm3.5_8.298) %>% 
  run_model_on_partitions(p_ch4_sets, ., ch4_binspec, plot_units="cm\u00B3/cm\u00B3", db_name = "hMOFs")
  # https://en.wikipedia.org/wiki/Unicode_subscripts_and_superscripts
p_ch4_vol$plots$parity_full %>% 
  rescale_ch4_parity()
ad_hoc_ch4_grids <- read_rds("BigData/Robj/adhoc_ch4.Rds")
ch4_base_hist <-
  ad_hoc_ch4_grids %>% 
  filter(id == "hMOF-0") %>% 
  plot_hist_bins(ch4_binspec)
p_beta_ch4_raw_models <- 
  list(`5.8 bar` = "ch4.cm3.cm3.5_8.298", `65 bar` = "ch4.cm3.cm3.65.298") %>% 
  map(mutate_col_to_gL, df=p_2bar_data) %>%
  map(function(x) run_model_on_partitions(p_ch4_sets, x, ch4_binspec)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat")
p_beta_ch4 <- 
  p_beta_ch4_raw_model %>% 
  overlay_cat_betas(ch4_base_hist, ., ch4_binspec, scaling = BETA_CH4_SCALING) %>% 
  replot_hist_and_beta(undo_scaling = BETA_CH4_SCALING)
p_beta_comb_ch4 <- 
  p_ch4_vol$trained_mod$mod %>% 
  coef_tbl() %>% 
  overlay_cat_betas(ch4_base_hist, ., ch4_binspec, scaling = BETA_CH4_SCALING/5.0) %>% 
  replot_hist_and_beta(undo_scaling = BETA_CH4_SCALING/5.0)

p_ch4_vol$plots %$%
  plot_grid(
    parity_testing %>% rescale_ch4_parity() + theme(axis.title.y = element_text(hjust=1.5)),  # Need to shift it down
    #parity_testing %>% rescale_ch4_parity() + ylab("Predicted capacity\n(LASSO, cm\u00B3/cm\u00B3)"),  # shrinks it weird, so let's just shift it
    p_beta_ch4 + theme(legend.position=c(0.25, 0.9), legend.direction="horizontal"),
    p_beta_comb_ch4 + theme(legend.position=c(0, 0.2), legend.direction="horizontal"),
    ncol = 1, nrow = 3,
    #rel_heights = c(1.0, 1.1/2.0),
    rel_heights=c(1.0/1.1, 1.0/2.0, 1.0/2.0),
    align = "v",
    labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
    ) %>% 
  #save_ben_fig("ch4_plots.png", nrow = 3, ncol = 1, base_aspect_ratio=1.1)
  save_plot(
    file.path(default_fig_dir, "ch4_plots.png"),
    .,
    base_width = 8.3 / 2.54,
    #base_height = 8.3 / 2.54 / c(1.1, 2.0, 2.0),
    base_height = 8.3 / 2.54 / 1.1,
    nrow = 1.0/1.1 + 1,  # Need to set the total height, not the actual number of rows here
    dpi = default_dpi,
  )

```

```{r main_textural}
# Models based on textural properties
# This analysis is currently unused for this paper, but the textural property import, etc., are referenced later in this document, so let's keep it and revisit the results in the follow-up.
# All of the "good" models so far have been based on methane.  Can we reproduce the good predictivity for CH4 using MLR?  Maybe the problem is like hydrogen storage, where high P is predicted by Chahine's rule
# * What to do about accessible vs. non-accessible properties?  Just use the hMOFs verbatim (plus density)?  Do a combined void fraction?
# Serves as a baseline for how well the energy histogram is working.
# Selected properties based on a quick literature review of ML:
# We should use rho, VF, VSA/GSA, and potentially LCD or other pore diameters.
# Cory Simon, EES 2015, 1190-1199: Density, VF, largest included sphere, AVSA
# Fernandez, JPCC 2013, 7681-7689: DPD, VF, GSA (and VSA)
# Pardakhti, ACS Comb Sci 2017, 7681-7689: VF, GSA, density, DPD, LCD, Interp. cap, Cat nets
# Plus chemical properties: element counts, metal type, degree of saturation, metallic %, ratios of atom types
# Thornton, Chem Mater 2017, 2844-2854: VF, density, adsorption energy (kJ/mol), pore diameter, GSA, VSA


fit_textural_model <- function(x_with_id_and_y_act, ...) {
  # Fits a LASSO/ridge regression model on a dataframe, subtracting the id column
  # and using the y_act column as the response variable.
  # Other parameters can be fed to fit_glmnet via the elipses
  fitted_mod <- 
    x_with_id_and_y_act %>% 
    select(-id) %>% 
    {fit_glmnet(select(., -y_act), .$y_act)}
  parity_plot(fitted_mod$y, pred_glmnet(fitted_mod, fitted_mod$orig_x)) %>% print
  fitted_mod
}

eval_textural_model <- function(glmnet_mod, x_with_id_and_y_act, ...) {
  # Runs a trained textural model on a data frame with the same column order (and y_act)
  # Returns the predicted data frame, and generates a parity plot and stats for convenience.
  
  # Adapted from pred_grid
  grid_ids <- x_with_id_and_y_act$id
  grid_y_act <- x_with_id_and_y_act$y_act
  grid_desc <- x_with_id_and_y_act %>% select(-id, -y_act)
  
  grid_y_pred <- pred_glmnet(glmnet_mod, grid_desc)
  
  pred_df <-
    tibble(id = grid_ids, y_act = grid_y_act, y_pred = grid_y_pred) %>%
    mutate(y_err = abs(y_act - y_pred)) %>% 
    bind_cols(grid_desc)
  
  pred_df %$% parity_plot(y_act, y_pred) %>% print
  pred_df %$% postResample(pred=y_pred, obs=y_act) %>% print

  pred_df
}

# Textural property base model
zeo_tsv_names <- c("filename", "LCD", "PLD", "avf", "navf", "density", "avsa", "agsa", "navsa", "nagsa")
zeo_hmof <- read_delim(
  "BigData/quest/PoreChar/all.tsv",
  col_names = zeo_tsv_names,
  col_types = "cddddddddd",  # parse_number doesn't process scientific notation, so we need to use dbl's
  skip = 1,
  delim = " "
  ) %>% 
  mutate(vf = avf + navf, vsa = avsa + navsa, gsa = agsa + nagsa) %>% 
  mutate(id = as.integer(str_sub(filename, 6)))
zeo_tobmof <- read_delim(
  "BigData/quest/TobPores/all.tsv",
  col_names = zeo_tsv_names,
  col_types = "cddddddddd",  # parse_number doesn't process scientific notation, so we need to use dbl's
  skip = 1,
  delim = " "
  ) %>% 
  mutate(vf = avf + navf, vsa = avsa + navsa, gsa = agsa + nagsa) %>% 
  mutate(id = filename)  # ToBaCCo id's start with tobmof, so it's already the correct format

hmof_textural_data <- 
  zeo_hmof %>% 
  inner_join(gcmc_data, by="id") %>% 
  mutate(y_act = va100bar - va2bar)
zeo_model <- 
  hmof_textural_data %>% 
  filter(id %in% hmof_hist_sets$training$id) %>%
  #select(id, y_act, LCD, PLD, avf, density, agsa, avsa) %>%
  #select(id, y_act, LCD, PLD, vf, density, gsa, vsa) %>% 
  select(id, y_act, vf, density, gsa, vsa) %>% 
  fit_textural_model()
expect_equal(nrow(zeo_model$orig_x), 1000)  # no longer true now that we've changed the hmof grids to be based on 2bar notated data
hmof_t_test_data <- 
  hmof_textural_data %>% 
  filter(id %in% hmof_hist_sets$testing$id) %>% 
  #select(id, y_act, LCD, PLD, vf, density, gsa, vsa)
  select(id, y_act, vf, density, gsa, vsa)
eval_textural_model(zeo_model, hmof_t_test_data)
coef_tbl(zeo_model$mod)

tob_textural <- 
  zeo_tobmof %>% 
  inner_join(select(tobacco_data, -vf, -gsa, -vsa), by="id") %>% 
  mutate(y_act = fh.h2.g.L.100.77 - fh.h2.g.L.2.77)
tob_t_model <- 
  tob_textural %>% 
  filter(id %in% tob_hist_sets$training$id) %>% 
  #select(id, y_act, LCD, PLD, vf, density, gsa, vsa) %>%
  select(id, y_act, vf, density, gsa, vsa) %>% 
  fit_textural_model()
expect_equal(nrow(tob_t_model$orig_x), 1000)
tob_t_test_data <- 
  tob_textural %>% 
  filter(id %in% tob_hist_sets$testing$id) %>% 
  #select(id, y_act, LCD, PLD, vf, density, gsa, vsa)
  select(id, y_act, vf, density, gsa, vsa)

plot_grid(
  eval_textural_model(zeo_model, hmof_t_test_data) %$% parity_plot(y_act, y_pred) %>% annotate_plot("hMOFs predicting hMOFs"),
  eval_textural_model(zeo_model, tob_t_test_data) %$% parity_plot(y_act, y_pred) %>% annotate_plot("hMOFs predicting ToBaCCo"),
  eval_textural_model(tob_t_model, hmof_t_test_data) %$% parity_plot(y_act, y_pred) %>% annotate_plot("ToBaCCo predicting hMOFs"),
  eval_textural_model(tob_t_model, tob_t_test_data) %$% parity_plot(y_act, y_pred) %>% annotate_plot("ToBaCCo predicting ToBaCCo"),
  ncol = 2, nrow = 2,
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>% 
  save_si_fig("textural_predictions.png", ncol = 2, nrow = 2)

coef_tbl(zeo_model$mod)
coef_tbl(tob_t_model$mod)
```


```{r ccdc_selection}
#```{r ccdc_selection, eval=FALSE, include=FALSE}
# Prepare list of top CCDC MOFs for additional screening
# no_cnps is imported when generating the CCDC parity plot above, from work in filter_ccdc_chemistry.R

if (!exists("ccdc_predicted")) {
  ccdc_predicted <- 
    ccdc_h2_grids %>% 
    pred_grid(mixed_partitioned_mod$trained_mod, ., default_binspec)
}

ccdc_predicted %>% arrange(desc(y_pred)) %>% View

ccdc_predicted %>% arrange(desc(y_pred)) %>% top_n(1000, y_pred) %>% arrange(id) %>% select(id) %>% write_tsv("BigData/Output/top_ccdc.txt", col_names=FALSE)  # A reasonable number of the top candidates.  For the paper, 50 g/L might be a better threshold, since it's only 200 structures and a nice round number.

# Get a list of the top 1000 CCDC MOFs
ccdc_top1000 <- ccdc_predicted %>% arrange(desc(y_pred)) %>% top_n(1000, y_pred)
# Actually, let's overwrite this list with a static version, finalized 2018-05-28
ccdc_top1000_dynamic_id <- ccdc_top1000$id
ccdc_top1000_static_id <- read_tsv("BigData/Output/top_ccdc_20180528_for_manuscript.txt", col_types="c", col_names="id")$id
ccdc_top1000 <- filter(ccdc_predicted, id %in% ccdc_top1000_static_id) %>% arrange(desc(y_pred))
expect_equal(nrow(ccdc_top1000), 1000)


# Get the difference of the two lists to submit to mateo for additional screening
old_screening <- read_tsv("BigData/Output/top_ccdc_20180122.txt", skip=0, col_names="id", col_types="c")$id
old2_screening <- read_tsv("BigData/Output/new_ccdc_20180131.txt", skip=0, col_names="id", col_types="c")$id  # only ~250
ccdc_top1000 %>% filter(!(id %in% old_screening) & !(id %in% old2_screening)) %>% arrange(id) %>% select(id) %>% write_tsv("BigData/Output/new_ccdc.txt", col_names=FALSE)  # only 11 in the top 750 in round three, none of which are in the top 600.  Get around to this if there's any remaining in the top 500 in the final model.


parse_vol_data <- function(filename) {
  read_tsv(
    filename,
    skip=1,
    col_names=c("id", paste0("fh.h2.g.L.", c("2.77", "2.77.err", "100.77", "100.77.err"))),
    col_types="cdddd"
    ) %>% 
  mutate_at(vars(starts_with("fh.h2")), funs(. * 2.0 / 22.4)) %>% 
  mutate(g.L = fh.h2.g.L.100.77 - fh.h2.g.L.2.77)
}
screening_gcmc <- 
  list(orig="ScreenCSD", extended="ExtraCSD2", fill3="ExtraCSD3", quest_mixed="../../quest/FillCCDC77K", quest_final="../../quest/FillCCDC77K/Round2-20180528") %>% 
  map_chr(~paste0("BigData/Mateo/EnergyGrid/", .x, "/volume.tsv")) %>% 
  #list("BigData/Mateo/EnergyGrid/ScreenCSD/volume.tsv") %>% #, "ExtraCSD2") %>% 
  map_dfr(parse_vol_data, .id = "src") %>% 
  distinct(id, .keep_all=TRUE)  # only keep the first instance when multiple simulations have been run
screening_ccdc_160 <- 
  read_tsv(
    "BigData/quest/ccdc-160k-volume.tsv",
    skip=1,
    col_names=c("id", "fh.h2.g.L.5.160", "fh.h2.g.L.5.160.err"),
    col_types="cdd"
    ) %>% 
  mutate_at(vars(starts_with("fh.h2")), funs(. * 2.0 / 22.4))
screening_gcmc <- screening_gcmc %>% left_join(screening_ccdc_160, by="id")

# Or, screening_gcmc %>% inner_join(ccdc_predicted, by="id") %$% parity_plot(g.L, y_pred)
screening_top_performance <- 
  ccdc_h2_grids %>%
  filter(id %in% screening_gcmc$id) %>%
  filter(id %in% ccdc_top1000$id) %>%  # In the paper, we'll only be looking at the top 1000 MOFs
  eval_test_grid(mixed_partitioned_mod$trained_mod, ., default_binspec, mutate(screening_gcmc, y_act=g.L))
screening_top_performance
screening_accuracy <- 
  screening_gcmc %>% 
  mutate(y_act = g.L) %>% 
  filter(id %in% ccdc_top1000$id) %>% 
  inner_join(ccdc_predicted, by="id") %>% 
  mutate(y_err = y_pred - y_act) %>% 
  arrange(desc(abs(y_err)))
View(screening_accuracy)
# Instead of a parity plot (which is misleading), maybe we should just evaluate the overall residuals
hist(screening_accuracy$y_err, xlab="Overprediction")
nrow(filter(screening_accuracy, y_act > 46)) / nrow(filter(screening_accuracy, y_pred > 46))
(screening_accuracy %>% 
  ggplot(aes(y_act, y_pred, id=id)) +
  geom_point() +
  parity_line +
  coord_fixed()
  ) %>% 
  ggplotly

# Which simulation points are still missing?
ccdc_top1000 %>% 
  filter(!(id %in% screening_accuracy$id)) %>% 
  select(id) %>%
  write_tsv("BigData/Output/missing_1000_ccdc.txt", col_names=FALSE)
  

# Finally, let's stratify by dimensionality using previous code deleted in 29a74040
# Check the MOF framework dimensionality from Zeo++.
ccdc_dimensionality <- read_tsv(
  "BigData/quest/DimsCCDC/all_dimensionality.tsv",
  col_types="ciiiiii"
  )
ccdc_dimensionality %$% expect_equal(segments, frameworks + molecules)
ccdc_dimensionality %$% expect_equal(frameworks, f1d + f2d + f3d)
p_top_ccdc_by_dim <- 
  screening_accuracy %>% 
  left_join(ccdc_dimensionality, by="id") %>% 
  mutate(has_2d = (f2d>0)) %>% 
  mutate(has_3d = (f3d>0)) %>% 
  mutate(dim_tag = ifelse(has_2d, "2D", "")) %>% 
  mutate(dim_tag = ifelse(has_3d, "3D", dim_tag)) %>% 
  mutate(dim_tag = ifelse(dim_tag=="", "No 2D or 3D", dim_tag)) %>% 
  ggplot(aes(y_act, y_pred, id=id)) +
  geom_point() + parity_line +
  coord_fixed() +
  facet_wrap(~dim_tag) +
  ggtitle("Dimensionality of framework")
ggplotly(p_top_ccdc_by_dim)
#p_top_ccdc_by_dim %>% save_si_fig("top_ccdc_by_dims.png", base_aspect_ratio=1.5)
(p_top_ccdc_by_dim$data %>% 
  ggplot(aes(y_pred - y_act, id=id)) +
  geom_histogram() +
  facet_wrap(~dim_tag, scales="free") +
  ggtitle("Dimensionality of framework") +
  xlab("Predicted - Actual Deliverable Capacity (g/L)")
  ) %>% 
  save_si_fig("top_ccdc_by_dims.png", base_aspect_ratio=2)


p_top_ccdc_by_dim$data %>% 
  filter(has_3d) %>% 
  nrow %>%
  paste("Number of 3D CIFs:", .) %>% 
  print
p_top_ccdc_by_dim$data %>% 
  filter(has_3d) %>% 
  select(id, y_act, y_pred) %>% 
  arrange(desc(y_act)) %>% 
  rownames_to_column("Rank")

# Stats for the overall database
ccdc_dimensionality %>% nrow
ccdc_dimensionality %>% mutate(has_3d = (f3d>0)) %>% filter(has_3d) %>% nrow
14493/54775  # 26.5% of the MOFs have a 3D framework
p_top_ccdc_by_dim$data %>% filter(has_3d) %>% nrow

# Send the list of top 3D candidates to Arun
top_3d_for_arun <- 
  p_top_ccdc_by_dim$data %>% filter(has_3d) %>%
  arrange(desc(y_act)) %>%
  select(id, y_act, y_pred, y_err, everything())
top_3d_for_arun %>% write_tsv("BigData/Output/top_gcmc_for_arun.tsv")
top_25_for_arun <- 
  top_3d_for_arun %>% 
  top_n(25, y_act) %>% 
  .$id %>% 
  str_sub(0, -10)
top_25_for_arun %>% cat(top_25_for_arun, sep="\n", file="Figs/TopCandidates/top_gcmc_refcodes.txt")
# Along with their histograms
for (arun_code in top_25_for_arun) {
  ccdc_h2_grids %>%
    filter(id==paste0(arun_code, "_stripped")) %>%
    {plot_hist_bins(., default_binspec) +
      scale_y_continuous("Density of sites")# +
      #annotate("text", 0.5, 0.01, hjust=0.5, vjust=0, label="E>0")
      } %>% 
    save_si_fig(paste0("TopCandidates/", arun_code, ".png"))
}
# Can get the SMILES using some code within BigData/:
# for i in $(cat ../Figs/TopCandidates/top_gcmc_refcodes.txt); do grep $i ../../mofid/src/ob_datadir/core.smi >> top_smiles_for_arun.txt; done
# It's not exact due to the lack of "_" but close enough


# Note: GUNFAW01 is one of the top structures in Thornton 2017 but has poor performance (6 g/L) in the RR
# due to a bulky Ru component in the pore (counterion, or possibly missed solvent?).
# Some of the other structures were noted the same, although many of the top candidates have long rod structures.


# Another quick test: how consistent are my GCMC simulations vs. Scotty's?
screening_gcmc %>% 
  rename(new.g.L = g.L) %>%
  inner_join(ccdc_gcmc, by="id") %>% 
  rename(old.g.L = g.L) %>%
  select(id, old.g.L, new.g.L) %>% 
  mutate(discrepancy = abs(new.g.L - old.g.L)) %>% 
  arrange(desc(discrepancy))

```

```{r ccdc_t_swing}
# Try a temperature swing (77 K -> 160 K) in addition to the pressure swing (100 bar -> 5 bar)
# First, let's begin with 77 K grids (instead of the 160 K).  It's less complex, and I've shown the 160 K grids not to be too important (especially if you train against 77 K grids, but even if you predict on the grids interchangeably)

# First check assumptions about the energy grids and desorption GCMC data we're importing
expect_equal(nrow(p_160k_5bar_data), 2500)
p_160k_5bar_data %>% 
  filter(id %in% unique(p_2bar_sets$training$id)) %>%
  nrow %>% 
  expect_equal(1000)
# Train the hMOF model.  Based on the p_beta_diff_t figure.
trained_160k_mod <- run_bin_model_spec(
  p_2bar_sets$training,
  mutate(p_160k_5bar_data, g.L = va100bar - fh.h2.g.L.5.160),
  default_binspec
  ) %>% 
  .$fitted_model %>% 
  .[[1]]
# Check the model fit, etc.
p_160k_testing <- 
  p_160k_5bar_data %>% 
  mutate(y_act = va100bar - fh.h2.g.L.5.160) %>% 
  eval_test_grid(trained_160k_mod, p_2bar_sets$testing, default_binspec, ., db_name="hMOFs")
p_160k_testing
plot_grid(
  p_160k_testing$plots$parity_testing,
  p_ccdc_parity %>% make_draft("TODO"),
  ncol = 2,
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>% 
  save_si_fig("tested_160k_model.png", ncol = 2)


if (!exists("ccdc_160k_predicted")) {
  ccdc_160k_predicted <- 
    ccdc_h2_grids %>% 
    pred_grid(trained_160k_mod, ., default_binspec)
}

# What do the new model coefficients look like?
coef_tbl(trained_160k_mod$mod)

# How much improvement do we get from desorbing at the higher temperature?
# Comparison between ranking?
ccdc_desorb_strategy <- 
  ccdc_160k_predicted %>% 
  rename(y_160k_pred = y_pred) %>% 
  mutate(y_160k_pred = ifelse(y_160k_pred < 0, NA, y_160k_pred)) %>%  # institute a floor on predictions to avoid illegible parity plots
  left_join(rename(ccdc_predicted, y_77k_pred = y_pred), by="id") %>% 
  mutate(r_160k = rank(desc(y_160k_pred)), r_77k = rank(desc(y_77k_pred))) %>% 
  left_join(ccdc_dimensionality, by="id") %>% 
  mutate(has_3d = (f3d > 0))

p_compare_strategies <- 
  ccdc_desorb_strategy %>% 
  ggplot(aes(y_77k_pred, y_160k_pred)) +
  geom_point() +
  parity_line +
  xlab("Deliverable capacity, 77 K (g/L)") +
  ylab("Deliverable capacity, 160 + 77 K (g/L)") +
  coord_fixed(0.5)
p_compare_strategies
p_compare_strategies +
  facet_wrap(~has_3d) +
  geom_point(color="red", data=filter(ccdc_desorb_strategy, id=="UPOZAB_stripped"))

ccdc_desorb_strategy %>% 
  ggplot(aes(y_160k_pred)) +
  geom_histogram()

p_top500_160k <- 
  ccdc_desorb_strategy %>%
  filter(y_160k_pred > 53)
nrow(p_top500_160k)
p_top500_160k %>% 
  filter(id %in% ccdc_top1000$id)

# What are the stats on the top 500?  Overlap with the 3D candidates from the other database?
nrow(p_top500_160k)
top_23_ids_isothermal <- 
  p_top_ccdc_by_dim$data %>% 
  filter(has_3d & y_act > 45)
filter(p_top500_160k, id %in% top_23_ids_isothermal$id)
filter(top_23_ids_isothermal, ! id %in% p_top500_160k$id) %>% select(id)  # "18 of the top 23 MOFs at isothermal conditions are also among the top 500 at the alternative conditions"


# Now, make an xy scatterplot for the two conditions using actual GCMC data
# TODO: come back to this part with (b) 1000 CCDC testing data and (c) the top 500 of each.
label_77k_vs_160k <- function(p) {
  p +
    xlab("Deliv. cap. w/ release at 2 bar, 77 K") +
    ylab("Deliv. cap. w/ release at 5 bar, 160 K")  
}

p_160k_xy_all <- 
  list(
    `2500 hMOFs` = mutate(p_160k_5bar_data, fh.h2.g.L.100.77 = va100bar, fh.h2.g.L.2.77 = va2bar),
    `CCDC MOFs (validation data)` = ccdc_gcmc,
    `Top CCDC MOFs` = screening_gcmc
    ) %>% 
  map(., ~ mutate(.x,
        dc.5bar.160k = fh.h2.g.L.100.77 - fh.h2.g.L.5.160,
        dc.2bar.77k = fh.h2.g.L.100.77 - fh.h2.g.L.2.77
        ) %$% 
      parity_plot(dc.2bar.77k, dc.5bar.160k)
    ) %>% 
  map(., ~ label_77k_vs_160k(.x)) %>% 
  map2(., names(.), ~annotate_plot(.x, .y))


plot_grid(
  p_160k_xy_all[[1]],
  p_160k_xy_all[[2]],
  p_160k_xy_all[[3]],
  ncol=3,
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>%
  save_si_fig("xy_160k_vs_77k.png", ncol = 3)


```


```{r ccdc_report}
# Report the top CSD candidates to a nice table in the SI
screening_3d <- 
  screening_accuracy %>% 
  left_join(ccdc_dimensionality, by="id") %>% 
  mutate(has_3d = (f3d>0)) %>% 
  filter(has_3d) %>% 
  arrange(desc(y_act))

# PAPER: View screening_3d and find that there are only 31 CIFs characterized as 3D by Zeo++, and
# only 23 have an "actual" capacity above 45 g/L.

# TODO: how many structures have an actual GCMC capacity more than 50 g/L?


# CCDC candidates containing a 3D framework and having a GCMC deliverable capacity > 45 g/L
screening_3d %>%
  filter(y_act > 45) %>% 
  mutate(Refcode = str_sub(id, 0, -10)) %>% 
  mutate(Visualization = paste0("\\includegraphics[height=3cm]{TopCCDC/", Refcode, "}")) %>% 
  select(Refcode, Visualization, y_act, y_pred) %>% # also consider VF, LCD, PLD, ASA
  format_column_digits(c("y_act", "y_pred"), ndigit=1) %>% 
  rename(
    `Predicted uptake (g/L) (will automatically add VF, LCD, PLD, and ASA once calculated)` = y_pred,
    `GCMC uptake (g/L)` = y_act
    ) %>% 
  save_fig_table_with_colnames("ccdc_screening.tex")

```

```{r mfu_mof}
# Figure describing MFU-4l MOF, adapted from the example case study isotherms
mfu_simulated <-
  read_xlsx(
    "BigData/Emails/high-T-GCMC-data-plots-20180501.xlsx",
    sheet = "highT",
    range = "F6:H17"
    ) %>% 
  full_join(read_xlsx(
    "BigData/Emails/high-T-GCMC-data-plots-20180501.xlsx",
    sheet = "highT",
    range = "B4:C17"
    ), by=c("P bar" = "P, bar")) %>% 
  gather("T", "mol.kg", ends_with(" K")) %>% 
  arrange(T, `P bar`) %>% 
  na.omit

mfu_fillin_simulated <- read_tsv(
  "BigData/quest/20180524_rerun_h2_296k.tsv",
  col_types = "---iidd",
  col_names = c("T K", "P kPa", "mol.kg", "mol.kg.err"),
  skip = 1
  ) %>% 
  mutate(`P bar` = `P kPa` / 100) %>% 
  mutate(T = str_c(`T K`, " K"))

mfu_fillin_simulated %>% 
  left_join(mfu_simulated, by=c("P bar", "T"), suffix=c(".fillin", ".orig")) %>% 
  filter(T=="298 K") %>% 
  mutate(gcmc_diff = mol.kg.fillin - mol.kg.orig)

# Since we've verified that the 298 K data are consistent between me and Scotty in the block above,
# let's generate a data frame only containing the data relevant to direct experimental comparison
mfu_expt_temperatures <- c("77 K", "160 K", "296 K")
mfu_simulated_consistent_t <- 
  mfu_fillin_simulated %>% select(`P bar`, T, mol.kg) %>% 
  bind_rows(mfu_simulated) %>% 
  filter(T %in% mfu_expt_temperatures)
  

mfu_pore_vol <- 1.3  # cm3/g from N2 experiment
mfu_pv_m3.kg <- 1.3 / 10^6 * 1000
mfu_expt <-
  read_xlsx(
    "BigData/Emails/20180504-expt-diff-T.xlsx",
    sheet = "Sheet1",
    range = "A1:E54"
    ) %>% 
  mutate(mol.kg = `XS expt (mol/kg)` + mfu_pv_m3.kg * `NIST rho (mol/m3)`)

mfu_dens <- 0.588  # g/cm3 from Zeo++ -- this matches the He pore volume from RASPA, but not the 1.3 from experiment!
mmol.g_to_g.L <- 1 / 1000 * 2.02 * mfu_dens * 1000

p_mfu_isotherm <- 
  mfu_expt %>% 
  mutate(T = str_c(`T (K)`, " K")) %>% 
  select(`P bar` = "P (bar)", "T", "mol.kg") %>%
  bind_rows(
    GCMC = mfu_simulated_consistent_t,
    Expt = .,
    .id = "source"
  ) %>%
  ggplot(aes(`P bar`, mol.kg)) +
  # geom_line() +
  geom_point(aes(color=source, shape=T)) +
  #scale_shape_manual(values=c(15, 0))  +  # hollow symbols for simulation, solid for experiment
  xlab("Pressure (bar)") + ylab("Uptake (mmol/g)") +
  theme(legend.position=c(-0.05,0.9)) +  # Or, to disable entirely, use: theme(legend.position="none") +
  geom_text(
    #data = tibble(T=mfu_expt_temperatures, `P bar`=90, mol.kg=c(37, 23, 2), source=I("black")),
    data = tibble(`P bar`=90, mol.kg=c(37, 23, 2)),
    aes(label=mfu_expt_temperatures),
    color="black"
    ) +
  scale_y_continuous(limits=c(0,52), sec.axis = sec_axis(~.*mmol.g_to_g.L, name="Uptake (g/L)", breaks=10*(0:6))) +
  scale_shape("") + scale_shape_discrete(guide=FALSE) +
  theme(
    #legend.box.background = element_rect(color = "black"),
    #legend.margin = margin(BETA_LEGEND_MARGIN, unit(5, "pt"), BETA_LEGEND_MARGIN, 0, "pt"),
    legend.title = element_blank()
    )

p_mfu_isotherm$data %>% arrange(source, T, `P bar`) %>%
  write_excel_csv("BigData/Output/plotted_mfu_data_export.csv")

plot_grid(
  ggdraw() + draw_image("Figs/mfu_viz.png", scale=0.9),
  p_mfu_isotherm,
  ncol=1,
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>%
  save_ben_fig("mfu_fig.png", nrow = 2, base_aspect_ratio = 1.75)

```





## Supporting Information for manuscript

This section is still largely TODO but at least can be done in part for some of the more important figures.

### Outline
First, some random figures that haven't been placed elsewhere for the SI yet.

```{r unsorted_si_figures}
# Run some predictions for Timur, using different metals for the identified MOF (March 9, 2018)
# 77 K P swing predictions:
ad_hoc_h2_grids %>% 
  pred_grid(mixed_partitioned_mod$trained_mod, ., default_binspec)

# 160 K P/T swing predictions (including 5 bar desorption)
ad_hoc_h2_grids %>% 
  pred_grid(trained_160k_mod, ., default_binspec)

```

### Database distributions

```{r si_distr}
# Geometric property distributions for the databases

# Let's read in the CoRE MOF database, but hide it by default in the next block
core_data <- 
  read_xlsx(
    "BigData/core_mof_1_si_c4ce02418d1.xlsx",
    sheet = "Table_S1",
    range = "B10:J5118",
    col_names = c("refcode", "metal", "density", "PLD", "LCD", "VSA (m2/cm3)", "GSA (m2/g)", "vf", "pore volume (cm3/g)")
    )

p_geom_distr <- 
  hmof_textural_data %>% 
  select(LCD, PLD, vf, density, gsa, vsa) %>% 
  bind_rows(
    hMOF = .,
    ToBaCCo = select(tob_textural, LCD, PLD, vf, density, gsa, vsa),
    #CoRE = select(core_data, LCD, PLD, vf, density, gsa="GSA (m2/g)", vsa="VSA (m2/cm3)"),
    .id = "db"
    ) %>% 
  rename(  # plotmath is very messy...
    `atop('Gravimetric surface', 'area'~(m^2/g))` = gsa,
    #`H2 deliverable\ncapacity (g/L)` = h2.deliv.77,
    `atop('Largest cavity', 'diameter'~(ring(A)))` = LCD,
    `atop('Pore limiting', 'diameter'~(ring(A)))` = PLD,
    `'Void fraction'` = vf,
    `'Density'~(g/cm^3)` = density,
    `atop('Volumetric surface', 'area'~(m^2/cm^3))` = vsa
    ) %>%
  select(-`'Density'~(g/cm^3)`) %>%   # behaves strangely in the plot
  gather("property", "value", -db) %>% 
  ggplot(aes(value)) +  # Trying to use ..density.. here results in strange behavior
  geom_histogram(bins=30) +
  # Idea related to https://stackoverflow.com/questions/11335836/increase-number-of-axis-ticks
  scale_x_continuous(breaks=scales::pretty_breaks(n=2)) +
  facet_grid(db ~ property, scales = "free", labeller = label_parsed) +
  background_grid()
p_geom_distr %>% save_si_fig("geom_distr.png", base_aspect_ratio=2.0)

p_y_distr <- 
  tobacco_data %>% 
  mutate(y_act = fh.h2.g.L.100.77 - fh.h2.g.L.2.77) %>% 
  select(g.L = y_act) %>% 
  bind_rows(
    ToBaCCo = .,
    hMOFs = select(gcmc_data, g.L),
    .id = "db"
    ) %>% 
  filter(g.L >= 0) %>% 
  ggplot(aes(g.L, ..density..)) +
  geom_histogram(bins=30) +
  facet_wrap(~db) +
  xlab("Hydrogen deliverable capacity, 77 K, 100 bar - 2 bar (g/L)") 
p_y_distr %>% save_si_fig("y_distr.png", base_aspect_ratio=1.6)
p_y_distr

p_x_distr_hmof <- plot_avg_with_distr(hmof_h2_grid, default_binspec, print_violin=TRUE)
p_x_distr_hmof
p_x_distr_tob <- plot_avg_with_distr(grids_h2, default_binspec, print_violin=TRUE)
p_x_distr_tob
plot_grid(
  p_x_distr_hmof + coord_cartesian(xlim = c(-15, 2)) + annotate("text", 0.5, 0.01, hjust=0.5, vjust=0, label="E>0"),
  p_x_distr_tob + coord_cartesian(xlim = c(-15, 2)) + annotate("text", 0.5, 0.01, hjust=0.5, vjust=0, label="E>0"),
  ncol=1,
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>%
  save_si_fig("x_distr.png", nrow = 2, base_aspect_ratio=1.6)

# How many samples are there for each bin in the training data?
# Answer: LASSO takes care of this automatically.
n_hmof_bin_samples <- 
  hmof_hist_sets$training %>%
  stepped_hist_spec(default_binspec) %>%
  select(-id) %>%
  filter(!(near(metric, 0))) %>%
  group_by(bin) %>% summarize(nsample = n()) %>%
  inner_join(bin_loc_from_spec(default_binspec), by="bin")
n_hmof_bin_samples

# In general, there should be an inverse relationship between void fraction and the infinite bin.
# The match will not be exact because the energy cutoffs are rather different.
p_vf_vs_inf <- 
  hmof_h2_grid %>% 
  stepped_hist_spec(default_binspec) %>% 
  filter(bin == "Inf") %>% 
  rename(`Inf bin` = metric) %>% 
  left_join(gcmc_data, by="id") %>%
  ggplot(aes(void.frac, `Inf bin`)) +
  geom_point(alpha=0.5) +
  xlim(0, 1) + ylim(0, 1) +
  xlab("Void fraction") + ylab("Repulsive bin")
p_vf_vs_inf
save_si_fig(p_vf_vs_inf, "vf_vs_inf.png")

```


### Considerations for molecular simulation

```{r si_simulation}
tobacco_data %>% 
  {list(
    `2 bar, 77 K` = ggplot(., aes(nofh.h2.g.L.2.77, fh.h2.g.L.2.77)),
    `100 bar, 77 K` = ggplot(., aes(lj.h2.g.L.100.77, fh.h2.g.L.100.77))
    )} %>% 
  map(
    ~ .x +
    geom_point() +
    coord_fixed(xlim = c(0, 70), y=c(0, 70)) +
    xlab("Lennard-Jones potential only") +
    ylab("With Feynman-Hibbs") +
    parity_line
    #annotate("text", x=10, y=50, label=names(.x))
    ) %>% 
  map2(
    .,
    names(.),
    function(x, y) {
      x + annotate("text", x=10, y=60, label=y)
    }
  ) %>% 
  {plot_grid(
    .[[1]], .[[2]],
    ncol=2,
    labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  )} %>% 
  save_si_fig("fh_correction.png", ncol=2)

# How much is this as a percentage?  ~12.6 to 14.3%, normalized to LJ
# tobacco_data %>% mutate(p_under = (lj.h2.g.L.100.77 - fh.h2.g.L.100.77) / lj.h2.g.L.100.77 * 100) %>% .$p_under %>% mean(na.rm=TRUE)
# tobacco_data %>% mutate(p_under = (nofh.h2.g.L.2.77 - fh.h2.g.L.2.77) / nofh.h2.g.L.2.77 * 100) %>% .$p_under %>% mean(na.rm=TRUE)

(list(
  `160 K grid` = filter(p_160k_grids, id %in% p_2bar_sets$testing$id),
  `77 K grid` = filter(p_2bar_sets$testing, id %in% p_160k_grids$id)
  ) %>% 
  map(~ eval_test_grid(hmof_partitioned_mod$trained_mod, .x, default_binspec, mutate(p_2bar_data, y_act = g.L))) %>% 
  map_dfr(~ .x$pred_df, .id = "grid") %>% 
  select(id, grid, y_pred) %>% 
  #spread(grid, y_pred) %$%
  #parity_plot(`160 K grid`, `77 K grid`)
  spread(grid, y_pred) %>%
  mutate(y_diff = `160 K grid` - `77 K grid`) %>% 
  ggplot(aes(y_diff)) +
  geom_histogram(bins = 30) +
  xlab("Different predictions from grids at 160 K - 77 K (g/L)")
  ) %>% 
  save_si_fig("fh_160k_energy_difference.png", base_aspect_ratio = 1.2)
```


### Model performance
```{r si_performance}
# Residuals of hMOFs and ToBaCCo
plot_resid_vs_act <- function(df) {
  ggplot(df, aes(y_act, y_act-y_pred)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    xlab("y") + ylab(expression('Residuals '~y-hat(y)))
}

plot_grid(
  hmof_from_mixed$plots$resid_normality,
  tob_from_mixed$plots$resid_normality,
  plot_resid_vs_act(hmof_partitioned_mod$pred_df),
  plot_resid_vs_act(tob_from_mixed$pred_df),
  ncol=2, nrow=2,
  align = "hv",
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>% 
  save_si_fig("residuals.png", ncol=2, nrow=2)

# Ranking of hMOF and ToBaCCo
plot_grid(
  hmof_partitioned_mod$plots$test_ranking,
  p_tob_from_hmof$plots$test_ranking,
  ncol=2,
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>% 
  save_si_fig("ranking.png", ncol=2)

# Error breakdown by topology
p_err_topology <- 
  tob_from_mixed$pred_df %>% 
  #left_join(tobacco_data, by="id") %>%  # only needed for p_tob_mod since it doesn't have composition data
  arrange(y_err) %>%  # consistently overplot the highest errors on top
  rename(`Prediction error` = y_err) %>% 
  ggplot(aes(y_act, y_pred, col=`Prediction error`)) + 
  geom_point() + parity_line +
  facet_wrap(~topology, nrow = 4) +
  coord_fixed() +
  scale_x_continuous(breaks=c(0, 40)) +  # this doesn't set the range, but rather the position of labels
  #scale_y_continuous(breaks=scales::pretty_breaks(n=2)) +
  scale_y_continuous(breaks=c(0, 40)) +
  scale_color_continuous(guide=guide_colorbar(title.position="top")) +
  theme(legend.position=c(0.75, 0.05), legend.direction="horizontal") +
  xlab("'Actual' uptake (GCMC, g/L)") +
  ylab("Predicted uptake (LASSO, g/L)")
p_err_topology %>% save_si_fig("err_by_topology.png", base_aspect_ratio = 1.5)

```

### Model regularization strategies
```{r si_regularization}
# Lambda plot from glmnet package.
# Viewing the source code with `plot.cv.glmnet` is very informative.
# See also an online vignette https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#lin
set_li <- function(x, label, value) {
  # Sets the value of a labelled list item then returns the full list.
  # Pipe-friendly assignment for lists.  Use quotes around the label title
  x[[label]] <- value
  x
}
hmof_partitioned_mod$trained_mod$cv_for_lambda %>% 
  set_li("lambda.1se", NA) %>%  # Remove the 1SE line for clarity
  set_li(., "nzero", rep("", length(.$nzero))) %>%  # Hide the number of variables used
  plot
#recordPlot() %>%  # Thanks, https://github.com/wilkelab/cowplot/issues/69
#  plot_to_gtable() %>%  # requires gridGraphics
#  save_si_fig("lambda.png")
# This works, but it'll be easier to figure out labels, aspect, etc. if we just replot it.
# After all, we have the plot.cv.glmnet source code easily available.
p_si_lambda_lambdamin <- hmof_partitioned_mod$trained_mod$cv_for_lambda$lambda.min
p_si_lambda_lambda1se <- hmof_partitioned_mod$trained_mod$cv_for_lambda$lambda.1se
p_si_lambda <- 
  hmof_partitioned_mod$trained_mod$cv_for_lambda %$% 
  qplot(log(lambda), cvm) +
  xlab("log(lambda)") +
  ylab("Mean-Squared Error") +
  geom_vline(xintercept=log(p_si_lambda_lambdamin), linetype="dotted")# +
  #geom_vline(xintercept=log(p_si_lambda_lambda1se), linetype="dashed")  # Hide to prevent figure clutter
p_si_lambda
p_si_lambda %>% save_si_fig("lambda.png")


# Combining LASSO, MLR, and ridge in a single figure
trained_model_regu <- list(
  Ridge = run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, default_binspec, 0, db_name = "hMOFs"),
  LASSO = run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, default_binspec, 1, db_name = "hMOFs"),
  MLR = run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, default_binspec, alpha=1, lambda=0, thresh=1e-16, db_name = "hMOFs")
)

trained_model_regu$MLR$plots$parity_testing <- 
  trained_model_regu$MLR$plots$parity_testing +
  ylab("Predicted uptake (MLR model)")  # fix the label

trained_model_regu %>% 
  #set_li("Ridge", NULL) %>%  # We only need to plot LASSO and MLR here
  map2(., names(.), function(x,y) {x$plots$parity_testing + annotate("text", x=45, y=20, label=y, size=6) + xlab("'Actual' capacity (GCMC, g/L)")}) %>% 
  {do.call(  # For future reference, purrr:invoke is probably what I was looking for as a wrapper
    plot_grid,
    c(., list(  # https://stackoverflow.com/questions/25962605/how-to-add-more-arguments-of-a-function-in-do-call-in-r
      ncol=3, nrow=1,
      labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
      )
    )
    )} %>% 
  save_si_fig("regu_parity.png", ncol=3, nrow=1)

trained_model_regu %>% 
  map_dfr(~ coef_tbl(.x$trained_mod$mod), .id="method") %>% 
  spread(method, beta) %>% 
  left_join(bin_loc_from_spec(default_binspec), by="bin") %>% 
  #select(-bin) %>% 
  mutate(Bin=paste(lower, "to", upper, "kJ/mol")) %>% 
  mutate(Bin=ifelse(is.na(lower), "(Intercept)", Bin)) %>% 
  arrange(lower) %>% 
  left_join(n_hmof_bin_samples, select(bin, nsample), by="bin") %>% 
  select(Bin, Ridge, LASSO, MLR, nsample) %>% 
  rename(`Number of nonzero samples` = nsample) %>%
  format_column_digits(c("Ridge", "LASSO", "MLR"), 0) %>% 
  save_fig_table_with_colnames("regu_coef.tex")

trained_model_regu %>%  # What are the lambda/alpha for the models?
  map_dfr(function(x) {list(lambda=x$trained_mod$lambda, alpha=x$trained_mod$alpha)}, .id="method")
  # If this is useful, gather then separate to add it to the previous DF, or a separate tex file.


# Saving tables of other model coefficients for model formulation analysis
p_beta_diff_p$layers[[2]]$data %>% 
  #mutate(beta = beta * BETA_H2_SCALING) %>%  # undo scaling from `overlay_cat_betas`, but no longer necessary with the rescaling
  select(lower, upper, cat, beta) %>% 
  spread(cat, beta) %>% 
  select(lower, upper, `2 bar`, `100 bar`) %>% 
  format_column_digits(c("2 bar", "100 bar"), 0) %>% 
  save_fig_table_with_colnames("coef_p.tex")

p_beta_diff_t$layers[[2]]$data %>% 
  #mutate(beta = beta * BETA_H2_SCALING) %>%  # See "coef_p.tex"
  select(lower, upper, cat, beta) %>% 
  spread(cat, beta) %>% 
  select(lower, upper, `77 K`, `160 K`) %>% 
  format_column_digits(c("77 K", "160 K"), 0) %>% 
  save_fig_table_with_colnames("coef_t.tex")

#p_beta_ch4$layers[[2]]$data %>% 
p_beta_ch4_raw_models %>% left_join(bin_loc_from_spec(ch4_binspec), by="bin") %>% 
  #mutate(beta = beta * BETA_CH4_SCALING) %>%  # See "coef_p.tex"
  select(lower, upper, cat, beta) %>% 
  spread(cat, beta) %>% 
  select(lower, upper, `5.8 bar`, `65 bar`) %>% 
  format_column_digits(c("5.8 bar", "65 bar"), 0) %>% 
    # from below
    mutate(Bin=paste(lower, "to", upper, "kJ/mol")) %>% 
    mutate(Bin=ifelse(is.na(lower), "(Intercept)", Bin)) %>% 
    arrange(lower) %>% 
    select(Bin, `5.8 bar`, `65 bar`) %>% 
  save_fig_table_with_colnames("coef_ch4_sep.tex")

# see regu_coef.tex above
n_ch4_bin_samples <- 
  p_ch4_sets$training %>%
  stepped_hist_spec(ch4_binspec) %>%
  select(-id) %>%
  filter(!(near(metric, 0))) %>%
  group_by(bin) %>% summarize(nsample = n()) %>%
  inner_join(bin_loc_from_spec(ch4_binspec), by="bin")
p_ch4_vol %>% 
  #map_dfr(~ coef_tbl(.x$trained_mod$mod), .id="method") %>% 
  .$trained_mod %>% 
  .$mod %>% 
  coef_tbl() %>% 
  #spread(method, beta) %>% 
  left_join(bin_loc_from_spec(ch4_binspec), by="bin") %>% 
  #select(-bin) %>% 
  mutate(Bin=paste(lower, "to", upper, "kJ/mol")) %>% 
  mutate(Bin=ifelse(is.na(lower), "(Intercept)", Bin)) %>% 
  arrange(lower) %>% 
  left_join(n_ch4_bin_samples, select(bin, nsample), by="bin") %>% 
  select(Bin, beta, nsample) %>% 
  rename(`Number of nonzero samples` = nsample) %>% 
  format_column_digits(c("beta"), 0) %>% 
  save_fig_table_with_colnames("coef_ch4.tex")

```

```{r nsample}
# Number of required samples in the bins, used to determine where the lowest bin should be established

# Get the occupancy of EnergyGrid bins, which will help systematically establish suitable ranges for the histograms
n_binspec <- default_binspec
n_binspec["from"] <- -20
n_binspec["to"] <- 0
nsample_h2_db <- 
  list(
    "hMOF" = hmof_hist_sets$training,
    "ToBaCCo" = tob_hist_sets$training,
    "CCDC Validation" = ccdc_h2_grids %>% filter(id %in% ccdc_gcmc$id)
  ) %>% 
  map_dfr(function(x) {
    x %>% 
      stepped_hist_spec(n_binspec) %>% 
      select(-id) %>% 
      filter(!near(metric, 0)) %>% 
      group_by(bin) %>% summarize(nsample = n()) %>% 
      inner_join(bin_loc_from_spec(n_binspec), by="bin") %>% 
      arrange(lower) %>%
      select(-loc, -bin)
    }, .id="Database") %>% 
  spread(Database, nsample) %>% 
  arrange(lower)
# If we look at weak adsorption (slightly positive regime), we still see occupancy from lots of MOFs.  But the absolute fraction is still tiny with low occupancy, so the beta values would probably be wildly variable.
# Now, export the nsample table to include in the SI as justification for the energy cutoffs.
# Comment that we want uniform bin spacing for its simplicity, but it's an area that could be explored in a future investigation/follow-up.

# Save the number of samples justification to determine the binspec in the first place.
nsample_h2_db %>% 
  mutate(Bin=paste(lower, "to", upper, "kJ/mol")) %>% 
  mutate(Bin=ifelse(is.na(lower), "(Intercept)", Bin)) %>% 
  select(-lower, -upper) %>% select(Bin, everything()) %>% 
  save_fig_table_with_colnames("nsample_lower.tex")


## Repeat the analysis for methane! ##
# Can't write a simple function for that, since the data sources are different for CH4 vs. H2, sadly
nch4_binspec <- ch4_binspec
nch4_binspec["from"] <- -40
nch4_binspec["to"] <- 0
nsample_ch4_db <- 
  list(
    "hMOF" = p_ch4_sets$training
  ) %>% 
  map_dfr(function(x) {
    x %>% 
      stepped_hist_spec(nch4_binspec) %>% 
      select(-id) %>% 
      filter(!near(metric, 0)) %>% 
      group_by(bin) %>% summarize(nsample = n()) %>% 
      inner_join(bin_loc_from_spec(nch4_binspec), by="bin") %>% 
      arrange(lower) %>%
      select(-loc, -bin)
    }, .id="Database") %>% 
  spread(Database, nsample) %>% 
  arrange(lower) %>% 
  rename(nsample = hMOF)
nsample_ch4_db %>% 
  mutate(Bin=paste(lower, "to", upper, "kJ/mol")) %>% 
  mutate(Bin=ifelse(is.na(lower), "(Intercept)", Bin)) %>% 
  select(-lower, -upper) %>% select(Bin, everything()) %>% 
  save_fig_table_with_colnames("nsample_ch4.tex")

# See previous block for the full table of H2/CH4 coefficients and nsample's used in the reported coef graph
```

```{r si_z_scores}
# Do we get the same conclusions if we preprocess the explanatory variables with z-scoring?
z_hmofs <- run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, default_binspec, db_name = "hMOFs", zscore = TRUE)
#print(z_hmofs)

# TODO: could consider z-scoring the MOF-5 results before making a base_hist
p_beta_z_diff_p <- 
  list(`100 bar` = "va100bar", `2 bar` = "va2bar") %>% 
  map(mutate_col_to_gL, df = gcmc_data) %>%
  map(function(x) run_model_on_partitions(hmof_hist_sets, x, default_binspec, zscore = TRUE)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec, 10) %>% 
  replot_hist_and_beta(undo_scaling=10)

plot_grid(
  z_hmofs$plots$parity_testing,
  p_beta_z_diff_p + theme(legend.position=c(0, 0.2), legend.direction="horizontal"),
  ncol = 2,
  rel_widths = c(1, 2.0/1.1),
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>% 
  save_si_fig("z_scored_hmofs.png", ncol = 2)

```


### Consistency between different datasets
hMOF vs. ToBaCCo, etc.

```{r si_consistency}
# Comparison of beta coefficients from ridge regression for the two databases.
# This code will have a similar structure to plotting multiple temperatures or pressures.
p_coef_compare <- 
  list(
    `hMOF` = hmof_partitioned_mod$trained_mod$mod,
    `ToBaCCo` = p_tob_mod$trained_mod$mod,
    `Mixed` = mixed_partitioned_mod$trained_mod$mod
    ) %>% 
  map_dfr(coef_tbl, .id="cat") %>%
  overlay_cat_betas(base_hist, ., default_binspec) %>% 
  replot_hist_and_beta(undo_scaling=BETA_H2_SCALING) +
  theme(legend.position=c(0.4, 0.2), legend.direction="horizontal")
p_coef_compare
p_coef_compare %>% save_si_fig("coef_compare.png", base_aspect_ratio = 1.5)

p_coef_compare$layers[[2]]$data %>% 
  # mutate(beta = beta * BETA_H2_SCALING) %>%  # see "coef_p.tex"
  select(lower, upper, cat, beta) %>% 
  spread(cat, beta) %>% 
  select(lower, upper, hMOF, ToBaCCo) %>% 
  format_column_digits(c("hMOF", "ToBaCCo"), 0) %>% 
  save_fig_table("coef_hmof_vs_tobacco.tex")


```


### Other verifications
```{r coarse_model}
# Are the models at the extremes of the hypertuning plot actually good fits to the data?
list(0.25, 2.5, 5.0, 7.5) %>% 
  map(.,
    function(x) list(
      model = run_bin_model(
        e_data = hmof_hist_sets$training,  # we no longer need hyperparameter data for other purposes
        y_with_id = hmof_y_to_join,
        step = x, width = x,
        bin_lims = c(default_binspec["from"], default_binspec["to"]),
        lambda = NULL, alpha = DEFAULT_ALPHA,
        align_bins = "downward"
        )$fitted_model[[1]],
      width = x
      )
  ) %>% 
  map(., function(x) eval_test_grid(
    x$model,
    hmof_hist_sets$testing,
    c(default_binspec[c("from", "to")], step=x$width, width=x$width),
    rename(hmof_y_to_join, y_act = g.L),
    align_bins = "downward"
    ))
# Interestingly, this works even for the super coarse bins, which only have four model terms: y-intercept, repulsive, and two parts of the energy as you'd expect.

#coarse_binspec <- c(default_binspec[c("from", "to")], step=10.0, width=10.0)
coarse_binspec <- c(from=-7.5, to=0.0, step=7.5, width=7.5)
coarse_hmof_model <- run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, coarse_binspec, db_name = "hMOFs", align_bins = "strict")
print(coarse_hmof_model)
coarse_tob_pred <- 
  tobacco_data %>% 
  mutate(y_act = fh.h2.g.L.100.77 - fh.h2.g.L.2.77) %>% 
  eval_test_grid(coarse_hmof_model$trained_mod, tob_hist_sets$testing, coarse_binspec, ., db_name = "ToBaCCo MOFs", align_bins="strict")
print(coarse_tob_pred)
plot_grid(
  coarse_hmof_model$plots$parity_testing,
  coarse_tob_pred$plots$parity_testing,
  ncol = 2,
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>% 
  save_si_fig("coarse_parity.png", ncol = 2)
coef_tbl(coarse_hmof_model$trained_mod$mod) %>%
  left_join(bin_loc_from_spec(coarse_binspec, align_bins="strict"), by="bin")
```

```{r less_training}
#list(50, 100, 500, 1000) %>%
less_trained_models <- 
  list(30, 50, 75, 100, 150, 200, 300, 400, 500, 1000) %>% 
  map(function(mapx) {
    hmof_hist_sets %>% 
    (function(x, nsample) {
      training_data <- x$training
      training_ids <- x$training$id %>% unique
      expect_equal(length(training_ids), 1000)  # rigorously DATA_SPLIT, but other things might be hardcoded, too
      sampled_ids <- sample(training_ids, nsample)
      x$training <- 
        training_data %>% 
        filter(id %in% sampled_ids)
      x
      })(., nsample=mapx) %>% 
    run_model_on_partitions(., hmof_y_to_join, default_binspec, db_name = "hMOFs")
    })
less_trained_stats <- 
  less_trained_models %>% 
  map_dfr(
    ~ data_frame(
      nfit = .x$trained_mod$nfit,
      prmse = .x$testing_fit["RMSE"],
      pmae = .x$testing_fit["MAE"],
      spearman = .x$test_spearman["y_pred", "y_act"],
      kendall = .x$test_kendall["y_pred", "y_act"]
    )
  )
print(less_trained_stats)
# Based on this analysis, as few as 100 samples is enough (Q2 of ~0.96, but 50 samples drops this down to 0.7 and clear outliers, when you use the default_binspec)
less_trained_stats %>% 
  format_column_digits(c("prmse", "pmae"), 1) %>% 
  format_column_digits(c("spearman", "kendall"), 2) %>% 
  rename(
    `Number of MOFs` = nfit,
    `RMSE` = prmse,
    `MAE` = pmae,
    `$r_{spearman}$` = spearman,
    `$r_{kendall}$` = kendall
    ) %>% 
  save_fig_table_with_colnames("small_n_stats.tex")


# Note, print returns its argument invisibly, so in a pipe it works like Bash's `tee`
# Consider extracting the first few to print, then otherwise saving the full set to the sI

```


```{r grid_convergence}
# Determine if energy grids have been sampled enough (small enough grid spacing)
# In the analysis, I used 1 AA for simplicity and a small file size.  See if that's enough.

# Import convergence testing data
fine_01_h2_grids <- read_rds("BigData/Robj/fine_01_h2.Rds")
fine_05_h2_grids <- read_rds("BigData/Robj/fine_05_h2.Rds")

# Start with our classic MOF-5 to see if there's a considerable difference. (double bar chart, or a side-by-side chart)
fine_mof_5_comparison <- 
  list(
    coarse10 = ad_hoc_h2_grids,
    fine05 = fine_05_h2_grids,
    fine01 = fine_01_h2_grids
    ) %>% 
  map_dfr(
    ~ .x %>% 
      filter(id == "hMOF-0") %>% 
      stepped_hist_spec(default_binspec) %>% 
      spread(key=bin, value=metric),
    .id = "spacing"
    )
fine_mof_5_comparison %>% 
  gather(bin, metric, -c("spacing", "id")) %>%
  inner_join(bin_loc_from_spec(default_binspec), by="bin") %>%
  arrange(lower) %>% 
  mutate(bin = factor(bin, levels = unique(bin))) %>% 
  mutate(bin_e = ifelse(spacing=="coarse10", paste(lower, "to", upper), "")) %>% 
  ggplot(aes(bin, metric, fill=spacing, label=bin_e)) +
  geom_col(position = position_dodge()) +
  geom_text(vjust=-0.5)

# How do model predictions compare?  Answer is they're 5 g/L different, which could explain some of the model spread.
list(
  coarse10 = ad_hoc_h2_grids,
  fine05 = fine_05_h2_grids,
  fine01 = fine_01_h2_grids
  ) %>%
  map(~ filter(.x, id == "hMOF-0")) %>% 
  map_dfr(
    ~ pred_grid(hmof_partitioned_mod$trained_mod, .x, default_binspec),
    .id = "spacing"
    )

# Then, extend this analysis to ~100 MOFs to see if it's systematic.
# sample(unique(hmof_hist_sets$training$id), 100)  # get the random list of 100
fine_num_ids <- 
  fine_05_h2_grids$id %>% 
  unique %>% 
  str_sub(6) %>% 
  as.numeric()
fine_diff_df <- 
  hmof_h2_grid %>% 
  filter(id %in% fine_num_ids) %>% 
  mutate(id = paste0("hMOF-", id)) %>% 
  rbind(filter(ad_hoc_h2_grids, id=="hMOF-0")) %>% 
  list(
    coarse10 = .,
    fine05 = fine_05_h2_grids,
    fine01 = fine_01_h2_grids
    ) %>%
  map_dfr(
    ~ pred_grid(hmof_partitioned_mod$trained_mod, .x, default_binspec),
    .id = "spacing"
  ) %>% 
  spread(spacing, y_pred) %>% 
  left_join(gcmc_data %>% select(id, y_act=g.L) %>% mutate(id = paste0("hMOF-",id)), by="id")
fine_diff_df
fine_diff_df %$%
  parity_plot(fine01, coarse10) +
  xlab("prediction from 0.1 Angstrom grid (fine)") +
  ylab("Prediction from 1.0 Angstrom grid (coarse)")
fine_diff_df %$% hist(coarse10 - fine01)
# Also look at a heatmap of errors to see if one model is necessarily better.
# If both models provide similar analysis (converged), we would expect them to systematically
# have the same error vs. GCMC, thus lie along the diagonal line.
# Off-diagonals indicate instances where the models have substantially different predictions.
# Added guides 3 g/L different to highlight points where the models diverge by more than 3 g/L.
fine_diff_df %>% 
  mutate(coarse_err = coarse10 - y_act, fine_err = fine01 - y_act) %>% 
  ggplot(aes(fine_err, coarse_err)) +
  #geom_density2d()
  stat_bin2d() +
  geom_hline(yintercept=0, linetype="dashed") + geom_vline(xintercept=0, linetype="dashed") +
  geom_abline(slope=1, intercept=0, linetype="dotted") +
  geom_abline(slope=1, intercept=3, linetype="dotted") +
  geom_abline(slope=1, intercept=-3, linetype="dotted") +
  coord_fixed()

# Finally, let's look back at old analysis from CCDC screening and number of points to see if it's insufficient sampling
fine_01_h2_grids %>% 
  group_by(id) %>% 
  summarize(num_pts = sum(counts)) %>% 
  arrange(num_pts) %>% 
  select(id, num_pts) %>% 
  ungroup() %>% 
  left_join(fine_diff_df, by="id") %>% 
  ggplot(aes(num_pts, coarse10 - fine01)) +  # Granted, num_pts will be 1000x as high as earlier
  geom_point()

# Now that we've looked at the finest 0.1 AA grid, consider an intermediate point, 0.5 AA, which will only
# be about one order of magnitude slower than 1.0 AA instead of three.
fine_diff_df %$% hist(fine05 - fine01)
p_fine_diff_hist <- 
  fine_diff_df %>%
  ggplot(aes(coarse10 - fine05)) +
  geom_histogram() + 
  xlab("Difference in predictions using grid spacings of 1.0 - 0.5 Angstroms")
print(p_fine_diff_hist)

# Substituting these two values in doesn't systematically predict better or worse
fine_diff_df %>% mutate(err10 = coarse10-y_act, err05=fine05-y_act) %>% ggplot(aes(err10, err05)) + geom_point() + parity_line

```

```{r grid_conv_models}
# Also test the grid spacing convergence by training models at a finer grid spacing
# of 0.5 AA instead of 1.0 AA.
hmof_fine_h2_grid <- read_rds("BigData/Robj/hmof_2500_fine05_h2.Rds")  # from definition of hmof_h2_grid
hmof_fine_h2_grid <- 
  hmof_fine_h2_grid %>%
  mutate(str_id = str_sub(id, 6, -1)) %>% # strip the hMOF- prefix
  mutate(id = as.integer(str_id)) %>% 
  select(-str_id)

# also filter this one out
simplified_fine_training_ids <- archived_training_ids$`hMOF 0.5AA`$training
simplified_fine_testing_ids <- archived_training_ids$`hMOF 0.5AA`$testing
simplified_fine_testing_ids <- sample(simplified_fine_testing_ids, 2250-length(simplified_fine_training_ids))
hmof_fine_h2_grid <- hmof_fine_h2_grid %>% filter(id %in% c(simplified_fine_training_ids, simplified_fine_testing_ids))

hmof_fine_hist_sets <- partition_data_subsets(hmof_fine_h2_grid, hmof_y_to_join, DATA_SPLIT, archived_training_ids$`hMOF 0.5AA`$training)
hmof_fine_partitioned_mod <- run_model_on_partitions(hmof_fine_hist_sets, hmof_y_to_join, default_binspec, db_name = "hMOFs")
print(hmof_fine_partitioned_mod)

plot_grid(
  hmof_fine_partitioned_mod$plots$parity_testing,
  p_fine_diff_hist,
  ncol = 2,
  labels=default_subfig_labels, label_size=12, hjust=0, vjust=1
  ) %>% 
  save_si_fig("coarse_grid_points.png", ncol = 2)

# This model shows that the fit isn't too much better when you use a finer grid spacing of 0.5 AA instead of 1.0,
# probably for the same reason that it's insensitive to bin width spacing: if it works with only four terms,
# why should the precision be important?  Model is robust and based on overall pore properties.
```


```{r toc_fig}
# Relevant plot, etc., for TOC figure
# Let's start with MOF-5 as a good representative example
# Based loosely on the poster's simplified figures
(hmof_partitioned_mod$pred_df %>% 
  #sample_n(100) %>% #.[1:100,] %>%
  .[1:100,] %>%
  ggplot(aes(y_act, y_pred, color=I("#0070C0"))) +
  geom_point(size = 3) +
  expand_limits(x = 0, y = 0) +
  scale_x_continuous(limits = c(0,60)) +
  scale_y_continuous(limits = c(0,60)) +
  coord_fixed() +
  parity_line +
  ylab("Machine\nlearning\nprediction") + xlab("GCMC simulation") +
  theme_cowplot(font_size = 14) +
  theme(axis.text = element_blank(), axis.ticks = element_blank()) +
  theme(axis.title.y = element_text(angle = 0)) +
  scale_size_manual(values=I(200))
) %>% 
  save_si_fig("for_toc_parity.png")

#base_hist + theme_diagram_min
base_hist %>% make_minimal() %>% save_si_fig("for_toc_histogram.png")

```


```{r diagnose_tobacco}
# What is the cause for the new error on the ToBaCCo parity plot?
# My suspicion is not enough granularity in the weakly attractive region (or lumping this together with repulsion), which is predicting adsorption when the binding is actually too weak.
# This would be apparent in the ToBaCCo large pores, so let's look at the Largest Cavity Diameter.
# The challenge then is that the bins might have to be different widths to account for this region--you can't just adjust the end points, because then the bin_above also shifts (e.g. from 0 to -0.25 as the "to" param)
p_tob_from_hmof$pred_df %>% ggplot(aes(y_act, y_pred, color=lcd)) + geom_point() + parity_line

# Also is this misprediction at low or high loadings?  My hypothesis is high loadings since it overpredicts the useful part of the void fraction.
## Adapted from p_beta_diff_p
gcmc_data %>% 
  mutate(g.L = va100bar) %>% 
  run_model_on_partitions(hmof_hist_sets, ., default_binspec) %>% 
  .$trained_mod %>%
  eval_test_grid(., tob_hist_sets$testing, default_binspec, mutate(tobacco_data, y_act=fh.h2.g.L.100.77)) %>% 
  .$plots %>% .$parity_testing %>%
  annotate_plot("ToBaCCo high P", "bottom left")
gcmc_data %>% 
  mutate(g.L = va2bar) %>% 
  run_model_on_partitions(hmof_hist_sets, ., default_binspec) %>% 
  .$trained_mod %>%
  eval_test_grid(., tob_hist_sets$testing, default_binspec, mutate(tobacco_data, y_act=fh.h2.g.L.2.77)) %>% 
  .$plots %>% .$parity_testing %>%
  annotate_plot("ToBaCCo low P", "bottom left")

# Conclusions: shifting alone doesn't work, because then the overfitting gets moved to the Inf repulsive bin.
# Using a smaller bin size gives nice agreement between ToBaCCo and hMOF coef's, except at strong adsorption (not enough data to make a reliable prediction?).
# Shifting by 0.5 kJ/mol and keeping the 1 kJ/mol bin width overfits ToBaCCo for huge pores (>40 Ang), but otherwise not too bad

test_h_to_tob_binspec <- function(binspec) {
  run_bin_model_spec(
    e_data = hmof_hist_sets$training,
    y_with_id = hmof_y_to_join,
    binspec = binspec,
    lambda = NULL, alpha = DEFAULT_ALPHA,
    align_bins = "strict"
    ) %>% 
    .$fitted_model %>% .[[1]] %>% 
    eval_test_grid(., tob_hist_sets$testing, binspec, mutate(tobacco_data, y_act=fh.h2.g.L.100.77-fh.h2.g.L.2.77))
}
test_h_to_tob_binspec(default_binspec) %>% 
  .$plots %>% .$parity_testing %>%
  annotate_plot("Default binspec", "bottom left")
test_h_to_tob_binspec(c(from=-9.75, to=0.0, step=0.75, width=0.75)) %>% 
  .$plots %>% .$parity_testing %>%
  annotate_plot("-9.75:0.75:0.0", "bottom left")
test_h_to_tob_binspec(c(from=-10.5, to=0.5, step=1.0, width=1.0)) %>% 
  .$plots %>% .$parity_testing %>%
  annotate_plot("-10.5:1.0:0.5", "bottom left")  # these params mostly work, though there's still a small lump due to high LCD
test_h_to_tob_binspec(c(from=-10.0, to=0.0, step=0.5, width=0.5)) %>% 
  .$plots %>% .$parity_testing %>%
  annotate_plot("-10.0:0.5:0.0", "bottom left")
test_h_to_tob_binspec(c(from=-10.0, to=0.0, step=0.5, width=0.5)) %>% 
  .$trained_mod %>% .$mod %>%
  coef_tbl() %>% overlay_cat_betas(base_hist, ., c(from=-10.0, to=0.0, step=0.5, width=0.5))

# Using a mixed model doesn't solve the problem completely.  There simply isn't enough granularity in the low attraction regime to account for the large pores.
tob_from_mixed %>% .$pred_df %>% ggplot(aes(y_act, y_act-y_pred, color=lcd)) + geom_point()
tob_from_mixed$plots$parity_testing
tob_from_mixed$plots$parity_full
base_hist %>% overlay_cat_betas(coef_tbl(tob_from_mixed$trained_mod$mod), default_binspec)

```

```{r si_other_tobacco_preds}
tob_hist_sets %>% 
  map(~filter(.x, !(near(upper, 0.0) | near(upper, -0.25)))) %>% 
  run_model_on_partitions(., tob_y_to_join, default_binspec, db_name = "ToBaCCo MOFs\n(empirical model)")
# Not used--it's just confusing, probbaly is missing the nearly exact 0 section, etc.

```


```{r opt_all_hmof_benchmarking}
if (FALSE) {
  full_hmof_grids <- read_rds("BigData/Robj/all_hmof_h2.Rds")
  full_hmof_predictions <- 
    full_hmof_grids %>% 
    pred_grid(hmof_partitioned_mod$trained_mod, ., default_binspec)
  full_hmof_accuracy <- 
    full_hmof_predictions %>%
    mutate(id=as.integer(str_sub(id, 6))) %>%
    left_join(gcmc_data, by="id") %>% 
    mutate(y_act = g.L)
  full_hmof_accuracy %$% parity_plot(y_act, y_pred, alpha=0.01) %>% save_si_fig("temp_full_hmof_parity.png")
  full_hmof_accuracy %>% ggplot(aes(y_act, y_pred-y_act)) + geom_point()
  full_hmof_accuracy %>% filter((y_pred-y_act)<30) %>% ggplot(aes(y_act, y_pred-y_act)) + geom_point()
  full_hmof_accuracy %>% filter((y_pred-y_act)<30) %>% {ggplot(., aes(y_pred-y_act)) + geom_histogram(bins = 30)} %>% save_si_fig("temp_full_hmof_resid_without_top_cluster.png")
  full_hmof_accuracy %$% postResample(pred=y_pred, obs=y_act)
}

```

```{r test_privileged_mofs, eval=FALSE, include=FALSE}
# Do a quick test on the David Sholl's list of 20 "privileged MOFs" which are optimal for over 60 separations
# https://doi.org/10.1002/cssc.201702289
library(wrapr)
library(forcats)
privileged_list <- qc(TEDGOA, XINHOT, OHAKIS, ZERNAN, AZIXUD, GOMRAC, GOMREG, ISEQIH, UVEXAV, EKOPOK, SUHHOT, KAXQIL, MOGVAG, LEGGEK, DOQGUM, VAHSIH, KAXQOR01, CIGYAU, XOKHAH, HUTTIA)
ccdc_h2_grids %>% 
  filter(id %in% str_c(privileged_list, "_stripped")) %>% 
  stepped_hist_spec(default_binspec) %>% 
  left_join(bin_loc_from_spec(default_binspec), by="bin") %>%
  mutate(bin=fct_reorder(bin, upper)) %>% 
  ggplot(aes(bin, metric, fill=id)) +
  geom_col(position = position_dodge()) + coord_cartesian(ylim=c(0, 0.05))
ccdc_h2_grids %>% 
  filter(id %in% str_c("TEDGOA", "_stripped")) %>% 
  plot_hist_bins(., default_binspec) + scale_y_continuous()

ccdc_h2_grids %>% filter(id %in% (ccdc_dimensionality %>% filter(f3d>0) %>% .$id)) %>% plot_avg_with_distr(., default_binspec, print_violin = TRUE) + coord_cartesian(ylim=c(0, 0.05))
```

```{r export_data_for_other_models, eval=FALSE, include=FALSE}
export_training_data <- function(partitioned_mod, filename) {
  partitioned_mod$trained_mod %$% bind_cols(y=y, orig_x) %>% write_csv(filename)
}
export_training_data(hmof_partitioned_mod, "Figs/ExportedData/hmof_training.csv")
export_training_data(p_tob_mod, "Figs/ExportedData/tobacco_training.csv")
export_training_data(mixed_partitioned_mod, "Figs/ExportedData/mixed_training.csv")

# Now get the testing data, using a combination of pred_grid/eval_test_grid.
export_testing_data <- function(test_grid, df_with_y_act, filename, binspec, align_bins="strict") {
  grid_desc <- test_grid %>%
    stepped_hist_spec(binspec, align_bins = align_bins) %>% 
    spread(key=bin, value=metric)
  results <- df_with_y_act %>% 
    select(id, y_act) %>% 
    inner_join(grid_desc, by="id") %>% 
    select(-id)
  results %>% write_csv(filename)
}
export_testing_data(hmof_hist_sets$testing, rename(hmof_y_to_join, y_act=g.L), "Figs/ExportedData/hmof_test.csv", default_binspec)
export_testing_data(tob_hist_sets$testing, rename(tob_y_to_join, y_act=g.L), "Figs/ExportedData/tobacco_test.csv", default_binspec)

```


```{r export_training_ids}
# Has the list of top MOFs changed since running the analysis on 2018-06-09?  Mention it if so
expect_equal(ccdc_top1000_dynamic_id %>% .[!(. %in% ccdc_top1000_static_id)] %>% length, 0)
# If it does change, comment out the line above (differences in random seed, environment, etc?)
# Otherwise, let's keep a list of which MOFs were used in the training data
ids_for_training_and_testing <- 
  list(
    "ToBaCCo H2" = tob_hist_sets,
    "hMOF H2" = hmof_hist_sets,
    "hMOF CH4" = p_ch4_sets,
    "ToBaCCo CH4" = tob_ch4_sets,
    "Mixed H2" = mixed_h2_hist_sets,
    "hMOF 0.5AA" = hmof_fine_hist_sets,
    "hMOF separate conditions" = p_2bar_sets
  ) %>% 
  map(~ list(
    training = unique(.x$training$id),
    testing = unique(.x$testing$id)
  ))
ids_for_training_and_testing %>% toJSON %>% cat(file="BigData/Output/ids_for_training_and_testing.json")
write_rds(ids_for_training_and_testing, "BigData/Output/ids_for_training_and_testing.Rds")
# Let's also write the important training sets to separate files for the SI
# Note the hMOF IDs are just the numberic part of Wilmer's filenames
ids_for_training_and_testing$`hMOF H2`$training %>% 
  cat(file="BigData/Output/ids_hmof_h2.txt", sep="\n")
ids_for_training_and_testing$`ToBaCCo H2`$training %>% 
  {filter(scotty_codes, id %in% .)} %>% 
  .$filename %>% 
  cat(file="BigData/Output/ids_tob_h2.txt", sep="\n")
ids_for_training_and_testing$`Mixed H2`$training %>% 
  {c(.[1:500], filter(scotty_codes, id %in% .[501:1000])$filename)} %>% 
  cat(file="BigData/Output/ids_mixed_h2.txt", sep="\n")
ccdc_gcmc$id %>%
  str_sub(0, -10) %>%  # remove the "_stripped" label
  cat(file="BigData/Output/ids_ccdc_validation.txt", sep="\n")
# we can convert these to a form for the paper using dos2unix and
# tr '\n' ',' < ids_tob_h2.txt | sed -e 's/,$//' | sed -e 's/,/, /g' | sed -e 's/_/-/g' > tob_for_paper.txt

```



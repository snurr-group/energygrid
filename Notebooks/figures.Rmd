---
title: "Generating figures for the manuscript"
author: "Ben Bucior"
date: "December 20, 2017"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r setup_data, include=FALSE}
# Has to be run in a separate block as above
source("Notebooks/setup_data.R")
library(plotly)
```

## Figures for manuscript: main text

Recreate (and export) figures in one block.  Possibly will be externalized into a separate script later.

```{r make_figures}
# Figures on adsorption strength (conceptual) and ML workflow (to update, but external)
# (likely generated within other software)

# Coefficients for ridge regression model
hmof_partitioned_mod <- run_model_on_partitions(hmof_hist_sets, hmof_y_to_join, default_binspec, 0)
trained_mod <- hmof_partitioned_mod$trained_mod
base_hist <- hmof_h2_grid %>% 
  filter(id == 71) %>% 
  plot_hist_bins(default_binspec)
base_hist %>% overlay_cat_betas(coef_tbl(trained_mod$mod), default_binspec)

# Beta coefficients for different pressures and temperatures
# For help on NSE in dplyr, see vignette("programming")
library(rlang)  # see also https://github.com/tidyverse/rlang/issues/116
mutate_col_to_gL <- function(df, newcol) {
  mutate(df, g.L = !!sym(newcol))
}
## Previously used for the CH4 plots, but no reason not to adapt it for T and P of H2
list(`100 bar` = "va100bar", `2 bar` = "va2bar") %>% 
  map(mutate_col_to_gL, df = gcmc_data) %>%
  map(function(x) run_model_on_partitions(hmof_hist_sets, x, default_binspec, 0)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec)
p_2bar_grids <- hmof_h2_grid %>% filter(id %in% p_2bar_data$id)
p_2bar_sets <- partition_data_subsets(p_2bar_grids, p_2bar_data, DATA_SPLIT)
list(`160 K` = "h2.g.L.2.160", `77 K` = "va2bar") %>% 
  map(mutate_col_to_gL, df = p_2bar_data) %>%
  map(function(x) run_model_on_partitions(p_2bar_sets, x, default_binspec, 0)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(base_hist, ., default_binspec)


# Model fit for hMOF ridge regression mdoel
print(hmof_partitioned_mod)

# Robustness to histogram binning strategy
hyper_tuned_hist <- read_rds("BigData/Robj/hyper_tuned.Rds")
plot_robustness_bin_gap <- 0.05
plot_robustness_beta_max <- 5
hyper_tuned_hist %>% 
  mutate(beta = ifelse(beta > plot_robustness_beta_max, plot_robustness_beta_max, beta)) %>% 
  mutate(beta = ifelse(beta < -plot_robustness_beta_max, -plot_robustness_beta_max, beta)) %>% 
  mutate(lower = lower + plot_robustness_bin_gap, upper = upper - plot_robustness_bin_gap) %>% 
  ggplot(aes(
    x = binwidth,
    ymin = lower,
    ymax = upper,
    color = beta
  )) +
  geom_linerange(size=2) +
  coord_flip() +
  scale_color_gradientn(
    colors = c("red", "darkgray", "blue"),
    limits = c(-plot_robustness_beta_max, plot_robustness_beta_max),
    breaks = c(-plot_robustness_beta_max, 0, plot_robustness_beta_max),
    labels = c(paste0(-plot_robustness_beta_max,"-"), "0", paste0(plot_robustness_beta_max,"+")),
    name=expression(beta[ridge])
    ) +
  labs(
    y = "Energy (kJ/mol)",
    x = "Width of histogram bins (kJ/mol)"
  )

# ToBaCCo plots
# TODO

# Methane predictions
p_ch4_vol <- p_2bar_data %>% 
  mutate(g.L = ch4.cm3.cm3.65.298 - ch4.cm3.cm3.5_8.298) %>% 
  run_model_on_partitions(p_ch4_sets, ., ch4_binspec, 0)
p_ch4_vol$plots$parity_full +
  scale_x_continuous(limits = c(0,250)) +
  scale_y_continuous(limits = c(0,250))
ch4_base_hist <- p_ch4_grids %>% 
  filter(dirname == "hMOF-71") %>%  # Using the same MOF as the sample H2 histogram
  plot_hist_bins(ch4_binspec)
list(`5.8 bar` = "ch4.cm3.cm3.5_8.298", `65 bar` = "ch4.cm3.cm3.65.298") %>% 
  map(mutate_col_to_gL, df=p_2bar_data) %>%
  map(function(x) run_model_on_partitions(p_ch4_sets, x, ch4_binspec, 0)) %>% 
  map(function(x) x$trained_mod$mod) %>% 
  map_dfr(coef_tbl, .id="cat") %>% 
  overlay_cat_betas(ch4_base_hist, ., ch4_binspec, scaling = 20.0)

# CCDC applicability, etc.
print("Note: revised below in another code block")
ccdc_h2_grids %>% 
  filter(id %in% ccdc_gcmc$id) %>% 
  eval_test_grid(trained_mod, ., default_binspec, mutate(ccdc_gcmc, y_act=g.L))
```


## Supporting Information for manuscript

This section is still largely TODO but at least can be done in part for some of the more important figures.

### Outline
Before working on SI revisions, I should take a step back to organize what content now belongs there.  Until then, here's a few new (random) figures I thought about that will be important for the (SI) story.

```{r unsorted_si_figures}
# In general, there should be an inverse relationship between void fraction and the infinite bin.
# The match will not be exact because the energy cutoffs are rather different.
grids_h2 %>% 
  stepped_hist_spec(default_binspec) %>% 
  filter(bin == "Inf") %>% 
  rename(`Inf bin` = metric) %>% 
  left_join(tobacco_data, by="id") %>%
  ggplot(aes(vf, `Inf bin`)) +
  geom_point(alpha=0.5)



```



## Preparing CSD MOFs for screening
In order to successfully analyze the CSD MOFs, we need to filter out the purely metallic structures.  Those are only pseudo-materials of specially spatially-arranged atoms.  Since they aren't MOFs, the model does not predict them well, either.

Finding MOFs without carbon isn't too difficult.  Open Babel has a convenient [command line option](https://openbabel.org/docs/dev/Command-line_tools/babel.html#append-option) to print out the molecular formula.  Then, we can use some basic text processing tools within R to explore which structures contain carbon, oxygen, etc.

```
cd /cygdrive/c/Users/Benjamin/Git/EnergyGrid/BigData/cifs_from_csddata;
obabel *.cif -ab -otxt --append "\tFORMULA" >> ../csd_formula.txt
# Note, the output has multiple warnings about atom labels like '0', '1', and '1.'.
```

```{r csd_formula, eval=FALSE, include=FALSE}
# Temporarily disable this (expensive) section until we're ready to do the analysis.
# It might actually belong in a separate notebook file if influencing other steps in the analysis procedure.
raw_csd_formulas <- read_tsv(
  "BigData/csd_formula.txt",
  col_names = c("cif", "str_formula"),
  col_types = "cc"
  )
library(CHNOSZ)

# We can convert a purrr list to data.frame using dplyr::bind_rows.
# See also https://github.com/tidyverse/dplyr/issues/1676 for the as.list invocation,
# which converts each named vector into its own list
# TODO: consider using `safely` to parse or hide errors
csd_formulas <- 
  raw_csd_formulas$str_formula %>% 
  as.list %>% 
  map(makeup) %>% 
  map(as.list) %>% 
  bind_rows %>% 
  bind_cols(raw_csd_formulas, .)

# Let's implement a less stringent form of the CoRE MOF requirements.
# Here, we'll require MOFs to contain at least one of C, N, P, and S.
# First, how different are the results if we filter by has_"just C" instead of the whole set?
csd_formulas %>% filter(is.na(C) & is.na(N) & is.na(P) & is.na(S)) %>% nrow
csd_formulas %>% filter(is.na(C)) %>% nrow
# Based on that analysis, it's probably okay for a first pass to be less restrictive
print("Structures to remove (without C, N, P, or S):")
csd_formulas <- 
  csd_formulas %>% 
  mutate(missing_cnps = (is.na(C) & is.na(N) & is.na(P) & is.na(S)))
no_cnps <- csd_formulas %>%
  filter(missing_cnps) %>% 
  select(cif, str_formula)
no_cnps
no_cnps %>% 
  select(cif) %>% 
  write_tsv("BigData/Output/csd_mofs_without_cnps.txt", col_names = FALSE)

# Now to look at the CCDC GCMC results and predictions without the pure metals
ccdc_h2_grids %>% 
  filter(id %in% ccdc_gcmc$id) %>% 
  filter(!(id %in% no_cnps$cif)) %>% 
  eval_test_grid(trained_mod, ., default_binspec, mutate(ccdc_gcmc, y_act=g.L))

if (!exists("ccdc_predicted")) {
  ccdc_predicted <- pred_grid(trained_mod, ccdc_h2_grids, default_binspec)
}
ccdc_predicted %>% .$y_pred %>% hist()
print("Summary of top 1000:")
ccdc_predicted %>% 
  filter(!(id %in% no_cnps$cif)) %>% 
  .$y_pred %>%
  sort(decreasing=TRUE) %>% .[1:1000] %>%
  summary

csd_formulas %>% 
  filter(!missing_cnps) %>% 
  select(cif, str_formula) %>% 
  rename(id = cif) %>% 
  inner_join(ccdc_predicted, by="id") %>% 
  arrange(desc(y_pred)) %>% top_n(1000, y_pred)
# Based on looking at a few of the top structures, the cleaned up versions are 1D rods, so not likely synthesizable.  But that's more a flaw of the database than the screening method, though it's possible the prediction method may not work well for non-MOF structures.
```



## TODO notes from meetings
Different things to consider for the analysis and manuscript.

### General
* Finish a second revision of the outline/manuscript
* Aggregate TODOs into this document to wrap up exploration and decisions
* Send Scotty list of top CCDC candidates once we have a (near-final?) model

### New with FH
* Set up calculations using Scotty's RASPA input file
* Convert RASPA ASCI_Grids to my old format from the C++ code: `cut -f 4 -d' ' asci_grid_H_com.grid | sed 's/^?$/Inf/'`
* Run the model using the revised calculations

### ToBaCCo errors
* Two separate models for high P and low P?  Might be easier for the model to understand the physics. (have I tried a parity plot of the combined model of separate?  That may be an interesting first step).
* Combining the 1000 + 1000 might be the way to go for the paper, since it also simplifies the story.

### Model formulation
* Check if glmnet's standardize flag is causing the issues with CV.
* Skip all standardization AND set the intercept to zero??
* Recalculate Q2 once we relax the penalty term to be lower.  Beyond RMSE, etc., how does Q2 vary for the models
* Do we need standardization?  If not, consider that it would be easier (for interpretability) not to normalize, and save the justification in the SI.

### Figures 
* Set up better infrastructure for automating figure generation
* Label Q2 (and other stats?) directly on the plot.
* Make these figures much bigger so we don't have to squint/zoom
* Set the size of the figures within R to get consistent font sizes and typefaces.
* "training on a combined database" should use independent data for training and testing
* Beta plot: consider drawing the background as the average histogram of the data.  Betas could be drawn as hlines instead of dots.
* Consider drawing in [cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html)

### Other from group meeting 12/7
* Beta interpretation: Why do strongly attractive energies not have a highly negative beta?  You'd expect it to be wasted pore volume like the "Inf" bin, thus have a similar beta.
* What about higher T for desorption?
* vf distribution of the ToBaCCo MOFs?  How similar is it to the hMOFs?
* How do the range of the hMOF and ToBaCCo energies compare?  Is the maximally attractive ToBaCCo region significantly lower than the hMOFs? (related: does our ToBaCCo energy go low enough?)
* Should the lowest ToBaCCo bins be included with the top bin?
* Can we optimize uptake for a given bin (calculating a maximum adsorption for an artificial pseudo-MOF using the histogram bins?)

### Code
* Place standardization as a flag for model fitting instead of repeated functions like `no_std_fit_glmnet` in the `no_standardization` code block
* Why does RASPA insist on making my H_com atom follow a CFC force field?  FH sidesteps this issue, but it may be important for a methane probe.

### Unfinished from tobacco_exploration.Rmd
* Regenerate plots using traditional textural property descriptors.  Maybe a hybrid model would fix training issues, since it's largely PLD driven?
* Re-implement prediction workflow in C++? (only necessary if we release an online interface)

#### Data that still needs a final version
* Wilmer's CH4 data to compare against the chemical intutition paper?

#### Other possible directions
* What sort of capacity could you get with a uniform background potential?
* Make a few parity plots from the Q2 analysis.  Is the good prediction consistent across histogram parameters?  How about for ToBaCCo, since it's likely more sensitive?
* Trying out traditional textural properties, or adding them to my set of energy-based features
* Would it be helpful to run PCA on the hMOF and/or ToBaCCo feature matricies?  Maybe the scores would show clustering between good/bad performance?
* Is there a relationship between the betas at different P/T?  That could help optimize operating parameters if we can interpolate predictive models.
* Do we have examples of similar histograms but completely different performance?


